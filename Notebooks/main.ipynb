{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Open Set Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_LABEL = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class MELDDataset(Dataset):\n",
    "#     def __init__(self, meld_dir, split, transform=None):\n",
    "#         train_df = pd.read_csv(\"../MELD_Dataset/train_sent_emo.csv\")\n",
    "#         labels = train_df['Emotion'].unique().tolist()\n",
    "#         self.label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "#         self.meld_dir = meld_dir\n",
    "#         self.transform = transform\n",
    "#         self.split = split\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'mel_spectrograms', f'{self.split}_img')\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'log_spectrogram', f'{self.split}_audio')\n",
    "\n",
    "#         # load and create sentence embeddings\n",
    "#         self.dialogues = self.load_dialogues()\n",
    "#         self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "#         sentences = self.dialogues['Utterance'].tolist()\n",
    "#         sentences = [text.replace(\"\\x92\", \"'\") for text in sentences]\n",
    "#         self.sentence_embeddings = self.sbert.encode(sentences, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "\n",
    "#         self.spectrograms = self.load_spectrograms()\n",
    "#         self.resnet_model = models.resnet50(pretrained=True)\n",
    "#         self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "#         self.feature_extractor.eval()\n",
    "\n",
    "#     def load_dialogues(self):\n",
    "#         dialogue_file = os.path.join(self.meld_dir, f'{self.split}_sent_emo.csv')\n",
    "#         dialogues = pd.read_csv(dialogue_file)\n",
    "#         return dialogues\n",
    "\n",
    "#     def load_spectrograms(self):\n",
    "#         images = os.listdir(self.img_path)\n",
    "#         return images\n",
    "\n",
    "#     def __len__(self):\n",
    "#         assert(len(self.sentence_embeddings) == len(self.spectrograms))\n",
    "#         return len(self.dialogues)\n",
    "\n",
    "#     def preprocess_img(self, img):\n",
    "#         preprocessor = transforms.Compose([\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "#         img_t =  preprocessor(img).to(device)\n",
    "#         return img_t\n",
    "\n",
    "#     def extract_audio_features_from_spectrogram(self, img):\n",
    "#         # Pass the input through the model\n",
    "#         with torch.no_grad():\n",
    "#             output = self.feature_extractor(img)\n",
    "#         return output\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.dialogues.iloc[idx]\n",
    "#         text = self.sentence_embeddings[idx]\n",
    "#         spectrogram_data = Image.open(os.path.join(self.img_path, f'dia{row[\"Dialogue_ID\"]}_utt{row[\"Utterance_ID\"]}.png'))\n",
    "#         spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "#         spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "#         spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "#         label = row['Emotion']\n",
    "#         label = torch.tensor(self.label_to_int[label])\n",
    "#         return text, spectrogram_data, label\n",
    "\n",
    "# train_meld = MELDDataset(\"../MELD_Dataset\", \"train\")\n",
    "# # test_meld = MELDDataset(\"../MELD_Dataset\", \"test\")\n",
    "# # dev_meld = MELDDataset(\"../MELD_Dataset\", \"dev\")\n",
    "\n",
    "# # concat all 3 datasets into 1 dataset\n",
    "# meld_dataset = train_meld # + test_meld + dev_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(meld_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IemocapDataset(Dataset):\n",
    "    def __init__(self, iemocap_dataset_full_path, iemocap_spectrogram_dir, iemocap_log_spectrogram_dir, is_closed_label_set_flag, labels_to_int, split, transform=None):\n",
    "        self.IEMOCAP_MAIN_FOLDER = os.path.join(iemocap_dataset_full_path,\"IEMOCAP_full_release\")\n",
    "        self.TRANSCRIPTION_FOLDER = os.path.join(\"dialog\", \"transcriptions\")\n",
    "        self.AUDIO_FOLDER = os.path.join(\"sentences\", \"wav\")\n",
    "        self.CATEGORICAL_LABELS_PATH = os.path.join(\"dialog\", \"EmoEvaluation\", \"Categorical\")\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.is_closed_label_set_flag = is_closed_label_set_flag\n",
    "        self.iemocap_spectrogram_dir = iemocap_spectrogram_dir\n",
    "        self.iemocap_log_spectrogram_dir = iemocap_log_spectrogram_dir\n",
    "        \n",
    "        self.errors = defaultdict(int)\n",
    "        self.unique_labels = []\n",
    "        self.audio_files = []\n",
    "        self.sentences_list = []\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.labels_to_int = labels_to_int\n",
    "\n",
    "        self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "        self.sentence_embeddings = self.sbert.encode(self.sentences_list, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "        \n",
    "        self.create_spectrograms(self.iemocap_spectrogram_dir)\n",
    "        self.create_log_spectrograms(self.iemocap_log_spectrogram_dir)\n",
    "        self.print_summary()\n",
    "\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "    def get_evaluator_filenames_with_video_file_prefix(self, input_list, prefix_value):\n",
    "        regex_pattern = re.compile(f'^{re.escape(prefix_value)}.*\\.txt$')\n",
    "        matching_strings = [s for s in input_list if regex_pattern.match(s)]\n",
    "        return matching_strings\n",
    "    \n",
    "    def get_utterance_to_evaluationCounter_mapping_from_evaluation_files(self, evaluation_files):\n",
    "        utterance_to_all_evaluations = {}\n",
    "\n",
    "        for evaluation_file in evaluation_files:\n",
    "            utterance_to_evaluationList = {}\n",
    "            with open(evaluation_file,'r') as f:\n",
    "                contents = f.read()\n",
    "                utterance_evaluations = contents.split(\"\\n\")\n",
    "                for evaluation in utterance_evaluations:\n",
    "                    evaluation = evaluation.strip()\n",
    "                    if(len(evaluation)==0):\n",
    "                        continue\n",
    "                    matches = re.findall(r':[^;]+;', evaluation)\n",
    "                    matches = [match[1:-1] for match in matches]\n",
    "                    utterance_to_evaluationList[evaluation.split()[0]] = matches\n",
    "            \n",
    "            # Combine lists from dict1\n",
    "            for key, value_list in utterance_to_evaluationList.items():\n",
    "                utterance_to_all_evaluations[key] = utterance_to_all_evaluations.get(key, []) + value_list\n",
    "\n",
    "        utterance_to_evaluationsCounter = {k:Counter(v).most_common(1)[0][0] for k,v in utterance_to_all_evaluations.items()}\n",
    "        return utterance_to_evaluationsCounter\n",
    "    \n",
    "    def is_label_a_closed_label(self,evaluation):\n",
    "        return evaluation in [\"Frustration\",\"Excited\",\"Neutral state\",\"Anger\",\"Sadness\",\"Happiness\"]\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for session_num in range(1,6):\n",
    "            for transcription_filename in os.listdir(os.path.join(self.IEMOCAP_MAIN_FOLDER,f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER)):\n",
    "                if(transcription_filename[0]!=\".\"): \n",
    "\n",
    "                    filename_without_extension = transcription_filename.split(\".\")[0]\n",
    "                    \n",
    "                    categorical_labels_folder_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH)\n",
    "                    evaluation_filenames = self.get_evaluator_filenames_with_video_file_prefix(os.listdir(categorical_labels_folder_full_path), filename_without_extension)\n",
    "                    evaluation_files_full_paths_for_this_file = [os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH, f) for f in evaluation_filenames]\n",
    "                    evaluations_per_utterance = self.get_utterance_to_evaluationCounter_mapping_from_evaluation_files(evaluation_files_full_paths_for_this_file)\n",
    "                    \n",
    "                    transcription_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER, transcription_filename) \n",
    "                    with open(transcription_file_full_path,'r') as f:\n",
    "                        contents = f.read()\n",
    "                        lines = contents.split(\"\\n\")\n",
    "\n",
    "                        # Iterate through utterances where every utterance looks like:\n",
    "                        # Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\n",
    "                        for line in lines:\n",
    "\n",
    "                            # Remove extra spaces and check if the line is not an empty link (usually at EOF)\n",
    "                            line = line.strip()\n",
    "                            if(len(line)==0):\n",
    "                                break\n",
    "\n",
    "                            # Remove idx of first space, ], -\n",
    "                            try:\n",
    "                                space_idx = line.index(\" \")\n",
    "                                timestampEndBracket_idx = line.index(\"]\")\n",
    "                                timestampHyphen_idx = line.index(\"-\")\n",
    "                            except:\n",
    "                                self.errors[\"Problematic Transcription Line\"]+=1\n",
    "                                continue\n",
    "                            else:\n",
    "                                audio_filename = line[:space_idx]        # output audio file name = utterance name\n",
    "                                text = line[timestampEndBracket_idx+3:]         # the transcription of the utterance\n",
    "                                evaluation = evaluations_per_utterance.get(audio_filename,\"KEY_ERROR\")\n",
    "                                if(evaluation==\"KEY_ERROR\"):\n",
    "                                    self.errors[\"Unavailable Label for an utterance\"]+=1\n",
    "\n",
    "                                utterance_audios_per_video_folder = audio_filename[:line.rindex('_')]       # Only need Ses01F_impro01 from Ses01F_impro01_F000\n",
    "                                audio_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.AUDIO_FOLDER, utterance_audios_per_video_folder, audio_filename+\".wav\")         # name of the video file\n",
    "\n",
    "                                if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True and self.is_label_a_closed_label(evaluation)==self.is_closed_label_set_flag):\n",
    "                                # if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True):    \n",
    "                                    self.audio_files.append(audio_file_full_path)\n",
    "                                    self.sentences_list.append(text)\n",
    "                                    dataset.append((text,audio_file_full_path,evaluation))\n",
    "                                    if evaluation not in self.unique_labels:\n",
    "                                        self.unique_labels.append(evaluation)\n",
    "        return dataset\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"SUMMARY:\\n\")\n",
    "        for k,v in self.errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    def create_spectrograms(self,iemocap_spectrogram_dir):\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_spectrogram_dir)\n",
    "        log_file_path = os.path.join(log_dir,'processed_files_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir,'error_files_spectrogram.log')\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        throttle_delay = 1 \n",
    "        def create_spectrogram(filename, audio_file_path, output_file_path):\n",
    "            y, sr = librosa.load(audio_file_path)\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(S_dB, sr=sr, fmax=8000)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_file_path)\n",
    "            plt.close()\n",
    "\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filename = self.audio_files[filenum]\n",
    "            if filename.endswith(\".wav\") and filename not in processed_files:\n",
    "\n",
    "                audio_file_path = os.path.join(filename)\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0])\n",
    "                try:\n",
    "                    create_spectrogram(filename, audio_file_path, output_file_path)\n",
    "                    processed_files.add(filename)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filename}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filename}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    \n",
    "    def create_log_spectrograms(self,iemocap_log_spectrogram_dir):\n",
    "        def log_specgram(audio, sample_rate, window_size=20,\n",
    "                        step_size=10, eps=1e-10):\n",
    "            nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "            noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "            freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', \n",
    "                                                    nperseg=nperseg, noverlap=noverlap, detrend=False)\n",
    "            return freqs, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "        def process_audio_file(filepath, output_dir):\n",
    "            sample_rate, audio = wavfile.read(filepath)\n",
    "            if audio.ndim > 1:\n",
    "                audio = audio.mean(axis=1)\n",
    "            _, spectrogram = log_specgram(audio, sample_rate)\n",
    "            plt.figure(figsize=(10, 4))  \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, os.path.splitext(os.path.basename(filepath))[0]+\".png\"))\n",
    "            plt.close()  \n",
    "\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_log_spectrogram_dir)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        log_file_path = os.path.join(log_dir, 'processed_files_log_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir, 'error_files_log_spectrogram.log')\n",
    "\n",
    "        throttle_delay = 1 \n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filepath = self.audio_files[filenum]\n",
    "            if filepath.endswith(\".wav\") and filepath not in processed_files:\n",
    "                try:\n",
    "                    process_audio_file(filepath, output_dir)\n",
    "                    processed_files.add(filepath)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filepath}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filepath}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for log spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_t =  preprocessor(img).to(device)\n",
    "        return img_t\n",
    "\n",
    "    def extract_audio_features_from_spectrogram(self, img):\n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            output = self.feature_extractor(img)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    from functools import lru_cache\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_audio_features(self, img_path):\n",
    "        spectrogram_data = Image.open(img_path)\n",
    "        spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "        spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "        spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "        return spectrogram_data\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, audio, label = self.dataset[idx]\n",
    "\n",
    "        text = self.sentence_embeddings[idx]\n",
    "\n",
    "        img_path = os.path.join(os.path.dirname(os.getcwd()),self.iemocap_spectrogram_dir,os.path.splitext(os.path.basename(audio))[0]+\".png\")\n",
    "        spectrogram_data = self.cached_audio_features(img_path)\n",
    "\n",
    "        if(self.is_closed_label_set_flag==False):\n",
    "            label = OTHER_LABEL\n",
    "        else:\n",
    "            label = self.labels_to_int[label]\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return text, spectrogram_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "100%|██████████| 245/245 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:00<00:00, 245134.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 77/77 [00:10<00:00,  7.58it/s]\n",
      "100%|██████████| 9794/9794 [00:00<00:00, 1637201.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9794/9794 [00:00<00:00, 1964281.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    }
   ],
   "source": [
    "IEMOCAP_FULL_PATH = os.path.join(os.path.dirname(os.getcwd()),\"IEMOCAP_full_release\")\n",
    "labels_to_int = {'Neutral state': 0,\n",
    "                'Frustration': 1,\n",
    "                'Anger': 2,\n",
    "                'Sadness': 3,\n",
    "                'Happiness': 4,\n",
    "                'Excited': 5,\n",
    "                'Surprise': 6,\n",
    "                'Fear': 7,\n",
    "                'Other': 8,\n",
    "                'Disgust': 9}\n",
    "\n",
    "openIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=False,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)\n",
    "closedIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=True,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2916, 5: 1976, 0: 1726, 2: 1269, 3: 1251, 4: 656})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([i[2] for i in closedIemocapDataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9794"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(closedIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_split(dataset, test_size):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset_labels = [item[2] for item in dataset]\n",
    "    stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Get the indices for training and testing sets\n",
    "    for train_idx, test_idx in stratified_splitter.split(indices, dataset_labels):\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, temp_dataset = get_stratified_split(closedIemocapDataset,0.2)\n",
    "val_dataset_1, test_dataset_1 = get_stratified_split(temp_dataset,0.5)\n",
    "val_dataset_2, test_dataset_2 = get_stratified_split(openIemocapDataset,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset_1+val_dataset_2\n",
    "test_dataset = test_dataset_1+test_dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTextEmotionModel(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AudioTextEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioTextEmotionModel, self).__init__()\n",
    "        ## sequential model with 2 layers, followed by dropout and relu layers and output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 768, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        combined = torch.cat([audio, text], axis=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "\n",
    "model = AudioTextEmotionModel(7)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.304245  [   64/ 7835]\n",
      "loss: 1.308634  [ 6464/ 7835]\n",
      "0.5079770261646458\n",
      "0.40253853127833183\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.335228  [   64/ 7835]\n",
      "loss: 1.358248  [ 6464/ 7835]\n",
      "0.5130823229100191\n",
      "0.4070716228467815\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.326890  [   64/ 7835]\n",
      "loss: 1.324119  [ 6464/ 7835]\n",
      "0.5207402680280792\n",
      "0.40797824116047143\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.295811  [   64/ 7835]\n",
      "loss: 1.266935  [ 6464/ 7835]\n",
      "0.5232929164007658\n",
      "0.41160471441523117\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.277601  [   64/ 7835]\n",
      "loss: 1.280897  [ 6464/ 7835]\n",
      "0.5294192724952138\n",
      "0.4170444242973708\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.309610  [   64/ 7835]\n",
      "loss: 1.280773  [ 6464/ 7835]\n",
      "0.5305679642629227\n",
      "0.4134179510426111\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.245987  [   64/ 7835]\n",
      "loss: 1.283735  [ 6464/ 7835]\n",
      "0.5352903637523931\n",
      "0.41523118766999095\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.270776  [   64/ 7835]\n",
      "loss: 1.281536  [ 6464/ 7835]\n",
      "0.5398851308232291\n",
      "0.41523118766999095\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.239457  [   64/ 7835]\n",
      "loss: 1.249406  [ 6464/ 7835]\n",
      "0.5433312061263561\n",
      "0.4170444242973708\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.233567  [   64/ 7835]\n",
      "loss: 1.258012  [ 6464/ 7835]\n",
      "0.5522654754307594\n",
      "0.4188576609247507\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.221313  [   64/ 7835]\n",
      "loss: 1.245856  [ 6464/ 7835]\n",
      "0.5489470325462668\n",
      "0.4188576609247507\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.262626  [   64/ 7835]\n",
      "loss: 1.222499  [ 6464/ 7835]\n",
      "0.5530312699425655\n",
      "0.41795104261106075\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.245609  [   64/ 7835]\n",
      "loss: 1.259263  [ 6464/ 7835]\n",
      "0.5578813018506701\n",
      "0.41795104261106075\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.243680  [   64/ 7835]\n",
      "loss: 1.257007  [ 6464/ 7835]\n",
      "0.5564773452456924\n",
      "0.4188576609247507\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.209329  [   64/ 7835]\n",
      "loss: 1.233806  [ 6464/ 7835]\n",
      "0.5560944479897894\n",
      "0.4125113327289211\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.238737  [   64/ 7835]\n",
      "loss: 1.178486  [ 6464/ 7835]\n",
      "0.5627313337587747\n",
      "0.42611060743427015\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.177460  [   64/ 7835]\n",
      "loss: 1.233832  [ 6464/ 7835]\n",
      "0.5606892150606254\n",
      "0.42792384406165007\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.209181  [   64/ 7835]\n",
      "loss: 1.205911  [ 6464/ 7835]\n",
      "0.5661774090619017\n",
      "0.42792384406165007\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.138603  [   64/ 7835]\n",
      "loss: 1.211561  [ 6464/ 7835]\n",
      "0.5687300574345884\n",
      "0.4242973708068903\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.211429  [   64/ 7835]\n",
      "loss: 1.203760  [ 6464/ 7835]\n",
      "0.5748564135290364\n",
      "0.4270172257479601\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.169737  [   64/ 7835]\n",
      "loss: 1.164179  [ 6464/ 7835]\n",
      "0.5725590299936184\n",
      "0.42339075249320035\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.170922  [   64/ 7835]\n",
      "loss: 1.138754  [ 6464/ 7835]\n",
      "0.5789406509253351\n",
      "0.4252039891205802\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.181934  [   64/ 7835]\n",
      "loss: 1.194775  [ 6464/ 7835]\n",
      "0.5771537970644544\n",
      "0.43064369900271987\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.184377  [   64/ 7835]\n",
      "loss: 1.140730  [ 6464/ 7835]\n",
      "0.5890236119974473\n",
      "0.4315503173164098\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.144566  [   64/ 7835]\n",
      "loss: 1.181243  [ 6464/ 7835]\n",
      "0.5925973197192087\n",
      "0.4315503173164098\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.117418  [   64/ 7835]\n",
      "loss: 1.182917  [ 6464/ 7835]\n",
      "0.5917038927887683\n",
      "0.42792384406165007\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.139408  [   64/ 7835]\n",
      "loss: 1.179341  [ 6464/ 7835]\n",
      "0.594639438417358\n",
      "0.4315503173164098\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.090258  [   64/ 7835]\n",
      "loss: 1.135684  [ 6464/ 7835]\n",
      "0.5991065730695597\n",
      "0.4242973708068903\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.138380  [   64/ 7835]\n",
      "loss: 1.135147  [ 6464/ 7835]\n",
      "0.595405232929164\n",
      "0.43517679057116954\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.071605  [   64/ 7835]\n",
      "loss: 1.079567  [ 6464/ 7835]\n",
      "0.5989789406509254\n",
      "0.4252039891205802\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.064579  [   64/ 7835]\n",
      "loss: 1.093185  [ 6464/ 7835]\n",
      "0.6063816209317167\n",
      "0.4315503173164098\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.097616  [   64/ 7835]\n",
      "loss: 1.098030  [ 6464/ 7835]\n",
      "0.610338225909381\n",
      "0.42883046237534\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.016606  [   64/ 7835]\n",
      "loss: 1.056723  [ 6464/ 7835]\n",
      "0.6116145500957243\n",
      "0.43245693563009974\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.996990  [   64/ 7835]\n",
      "loss: 1.083545  [ 6464/ 7835]\n",
      "0.6097000638162093\n",
      "0.43336355394378967\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.081508  [   64/ 7835]\n",
      "loss: 1.067165  [ 6464/ 7835]\n",
      "0.6223356732610082\n",
      "0.4397098821396192\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.040979  [   64/ 7835]\n",
      "loss: 1.072327  [ 6464/ 7835]\n",
      "0.624122527121889\n",
      "0.4496826835902085\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.057504  [   64/ 7835]\n",
      "loss: 1.037057  [ 6464/ 7835]\n",
      "0.6358647096362476\n",
      "0.44152311876699907\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.963557  [   64/ 7835]\n",
      "loss: 1.060708  [ 6464/ 7835]\n",
      "0.6450542437779196\n",
      "0.4514959202175884\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.022671  [   64/ 7835]\n",
      "loss: 1.046405  [ 6464/ 7835]\n",
      "0.644798978940651\n",
      "0.4369900271985494\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.995732  [   64/ 7835]\n",
      "loss: 0.973963  [ 6464/ 7835]\n",
      "0.6482450542437779\n",
      "0.45330915684496825\n"
     ]
    }
   ],
   "source": [
    "def accuracy(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        predicted = torch.argmax(pred,dim=1).cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "    return total_correct/size\n",
    "        # f1 = f1_score(torch.argmax(pred,dim=1).cpu(),label.cpu(),average=\"micro\")\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        loss = loss_fn(pred, label)\n",
    "        # acc = accuracy_score(torch.argmax(pred,dim=1).cpu(), label.cpu())\n",
    "        # f1 = f1_score(torch.argmax(pred,dim=1).cpu(),label.cpu(),average=\"micro\")\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(text)\n",
    "            # print(f\"loss: {loss:>7f}\\t\\tAccuracy: {acc:>7f}\\t\\tF1 Score: {f1:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(accuracy(train_dataloader,model))\n",
    "    print(accuracy(test_dataloader,model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_labels = []\n",
    "# for text, spectrogram_data, label in test_dataloader:\n",
    "#     all_labels = all_labels + list(label)\n",
    "#     print (len(all_labels))\n",
    "#\n",
    "# Counter(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "0 9\n",
      "1 12\n",
      "0 7\n",
      "0 3\n",
      "2 12\n",
      "0 5\n",
      "0 5\n",
      "0 3\n",
      "0 6\n",
      "0 6\n",
      "0 7\n",
      "0 9\n",
      "0 9\n",
      "2 9\n",
      "0 5\n",
      "0 8\n",
      "0 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45330915684496825"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_dropout_to_train(eval_model):\n",
    "    for module in eval_model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def predict(label, model, text, spectrogram_data, n_simulations=100, threshold=0.5, other_label=OTHER_LABEL):\n",
    "    predictions = [torch.argmax(model(spectrogram_data, text), dim=1).float() for _ in range(n_simulations)]\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    # Calculate mode values along axis 0\n",
    "    mode_values, _ = torch.mode(predictions, dim=0)\n",
    "\n",
    "    # Calculate the count of each mode value\n",
    "    mode_counts = torch.zeros_like(mode_values)\n",
    "    for i, mode_val in enumerate(mode_values):\n",
    "        mode_counts[i] = torch.sum(predictions[:, i] == mode_val)\n",
    "\n",
    "    mode_values[mode_counts <= threshold*n_simulations] = other_label\n",
    "    actual_other_label_counts = 0\n",
    "    correct_other_label = 0\n",
    "    for i in range(len(label)):\n",
    "        l = label[i]\n",
    "        if l==6:\n",
    "            actual_other_label_counts+=1\n",
    "            if mode_values[i]==6:\n",
    "                correct_other_label+=1\n",
    "    print(correct_other_label,actual_other_label_counts)\n",
    "    return mode_values\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    # After setting the model to evaluation mode, call this function\n",
    "    model.eval()\n",
    "    set_dropout_to_train(model)\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        predicted = predict(label, model, text, spectrogram_data).cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "    return total_correct/size\n",
    "\n",
    "evaluate(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn pytorch basic with some basic models and datasets\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/nnqs_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
