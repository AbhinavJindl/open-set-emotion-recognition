{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Open Set Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_LABEL = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class MELDDataset(Dataset):\n",
    "#     def __init__(self, meld_dir, split, transform=None):\n",
    "#         train_df = pd.read_csv(\"../MELD_Dataset/train_sent_emo.csv\")\n",
    "#         labels = train_df['Emotion'].unique().tolist()\n",
    "#         self.label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "#         self.meld_dir = meld_dir\n",
    "#         self.transform = transform\n",
    "#         self.split = split\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'mel_spectrograms', f'{self.split}_img')\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'log_spectrogram', f'{self.split}_audio')\n",
    "\n",
    "#         # load and create sentence embeddings\n",
    "#         self.dialogues = self.load_dialogues()\n",
    "#         self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "#         sentences = self.dialogues['Utterance'].tolist()\n",
    "#         sentences = [text.replace(\"\\x92\", \"'\") for text in sentences]\n",
    "#         self.sentence_embeddings = self.sbert.encode(sentences, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "\n",
    "#         self.spectrograms = self.load_spectrograms()\n",
    "#         self.resnet_model = models.resnet50(pretrained=True)\n",
    "#         self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "#         self.feature_extractor.eval()\n",
    "\n",
    "#     def load_dialogues(self):\n",
    "#         dialogue_file = os.path.join(self.meld_dir, f'{self.split}_sent_emo.csv')\n",
    "#         dialogues = pd.read_csv(dialogue_file)\n",
    "#         return dialogues\n",
    "\n",
    "#     def load_spectrograms(self):\n",
    "#         images = os.listdir(self.img_path)\n",
    "#         return images\n",
    "\n",
    "#     def __len__(self):\n",
    "#         assert(len(self.sentence_embeddings) == len(self.spectrograms))\n",
    "#         return len(self.dialogues)\n",
    "\n",
    "#     def preprocess_img(self, img):\n",
    "#         preprocessor = transforms.Compose([\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "#         img_t =  preprocessor(img).to(device)\n",
    "#         return img_t\n",
    "\n",
    "#     def extract_audio_features_from_spectrogram(self, img):\n",
    "#         # Pass the input through the model\n",
    "#         with torch.no_grad():\n",
    "#             output = self.feature_extractor(img)\n",
    "#         return output\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.dialogues.iloc[idx]\n",
    "#         text = self.sentence_embeddings[idx]\n",
    "#         spectrogram_data = Image.open(os.path.join(self.img_path, f'dia{row[\"Dialogue_ID\"]}_utt{row[\"Utterance_ID\"]}.png'))\n",
    "#         spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "#         spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "#         spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "#         label = row['Emotion']\n",
    "#         label = torch.tensor(self.label_to_int[label])\n",
    "#         return text, spectrogram_data, label\n",
    "\n",
    "# train_meld = MELDDataset(\"../MELD_Dataset\", \"train\")\n",
    "# # test_meld = MELDDataset(\"../MELD_Dataset\", \"test\")\n",
    "# # dev_meld = MELDDataset(\"../MELD_Dataset\", \"dev\")\n",
    "\n",
    "# # concat all 3 datasets into 1 dataset\n",
    "# meld_dataset = train_meld # + test_meld + dev_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(meld_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IemocapDataset(Dataset):\n",
    "    def __init__(self, iemocap_dataset_full_path, iemocap_spectrogram_dir, iemocap_log_spectrogram_dir, is_closed_label_set_flag, labels_to_int, split, transform=None):\n",
    "        self.IEMOCAP_MAIN_FOLDER = os.path.join(iemocap_dataset_full_path,\"IEMOCAP_full_release\")\n",
    "        self.TRANSCRIPTION_FOLDER = os.path.join(\"dialog\", \"transcriptions\")\n",
    "        self.AUDIO_FOLDER = os.path.join(\"sentences\", \"wav\")\n",
    "        self.CATEGORICAL_LABELS_PATH = os.path.join(\"dialog\", \"EmoEvaluation\", \"Categorical\")\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.is_closed_label_set_flag = is_closed_label_set_flag\n",
    "        self.iemocap_spectrogram_dir = iemocap_spectrogram_dir\n",
    "        self.iemocap_log_spectrogram_dir = iemocap_log_spectrogram_dir\n",
    "        \n",
    "        self.errors = defaultdict(int)\n",
    "        self.unique_labels = []\n",
    "        self.audio_files = []\n",
    "        self.sentences_list = []\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.labels_to_int = labels_to_int\n",
    "\n",
    "        self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "        self.sentence_embeddings = self.sbert.encode(self.sentences_list, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "        \n",
    "        self.create_spectrograms(self.iemocap_spectrogram_dir)\n",
    "        self.create_log_spectrograms(self.iemocap_log_spectrogram_dir)\n",
    "        self.print_summary()\n",
    "\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "    def get_evaluator_filenames_with_video_file_prefix(self, input_list, prefix_value):\n",
    "        regex_pattern = re.compile(f'^{re.escape(prefix_value)}.*\\.txt$')\n",
    "        matching_strings = [s for s in input_list if regex_pattern.match(s)]\n",
    "        return matching_strings\n",
    "    \n",
    "    def get_utterance_to_evaluationCounter_mapping_from_evaluation_files(self, evaluation_files):\n",
    "        utterance_to_all_evaluations = {}\n",
    "\n",
    "        for evaluation_file in evaluation_files:\n",
    "            utterance_to_evaluationList = {}\n",
    "            with open(evaluation_file,'r') as f:\n",
    "                contents = f.read()\n",
    "                utterance_evaluations = contents.split(\"\\n\")\n",
    "                for evaluation in utterance_evaluations:\n",
    "                    evaluation = evaluation.strip()\n",
    "                    if(len(evaluation)==0):\n",
    "                        continue\n",
    "                    matches = re.findall(r':[^;]+;', evaluation)\n",
    "                    matches = [match[1:-1] for match in matches]\n",
    "                    utterance_to_evaluationList[evaluation.split()[0]] = matches\n",
    "            \n",
    "            # Combine lists from dict1\n",
    "            for key, value_list in utterance_to_evaluationList.items():\n",
    "                utterance_to_all_evaluations[key] = utterance_to_all_evaluations.get(key, []) + value_list\n",
    "\n",
    "        utterance_to_evaluationsCounter = {k:Counter(v).most_common(1)[0][0] for k,v in utterance_to_all_evaluations.items()}\n",
    "        return utterance_to_evaluationsCounter\n",
    "    \n",
    "    def is_label_a_closed_label(self,evaluation):\n",
    "        return evaluation in [\"Frustration\",\"Excited\",\"Neutral state\",\"Anger\",\"Sadness\",\"Happiness\"]\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for session_num in range(1,6):\n",
    "            for transcription_filename in os.listdir(os.path.join(self.IEMOCAP_MAIN_FOLDER,f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER)):\n",
    "                if(transcription_filename[0]!=\".\"): \n",
    "\n",
    "                    filename_without_extension = transcription_filename.split(\".\")[0]\n",
    "                    \n",
    "                    categorical_labels_folder_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH)\n",
    "                    evaluation_filenames = self.get_evaluator_filenames_with_video_file_prefix(os.listdir(categorical_labels_folder_full_path), filename_without_extension)\n",
    "                    evaluation_files_full_paths_for_this_file = [os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH, f) for f in evaluation_filenames]\n",
    "                    evaluations_per_utterance = self.get_utterance_to_evaluationCounter_mapping_from_evaluation_files(evaluation_files_full_paths_for_this_file)\n",
    "                    \n",
    "                    transcription_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER, transcription_filename) \n",
    "                    with open(transcription_file_full_path,'r') as f:\n",
    "                        contents = f.read()\n",
    "                        lines = contents.split(\"\\n\")\n",
    "\n",
    "                        # Iterate through utterances where every utterance looks like:\n",
    "                        # Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\n",
    "                        for line in lines:\n",
    "\n",
    "                            # Remove extra spaces and check if the line is not an empty link (usually at EOF)\n",
    "                            line = line.strip()\n",
    "                            if(len(line)==0):\n",
    "                                break\n",
    "\n",
    "                            # Remove idx of first space, ], -\n",
    "                            try:\n",
    "                                space_idx = line.index(\" \")\n",
    "                                timestampEndBracket_idx = line.index(\"]\")\n",
    "                                timestampHyphen_idx = line.index(\"-\")\n",
    "                            except:\n",
    "                                self.errors[\"Problematic Transcription Line\"]+=1\n",
    "                                continue\n",
    "                            else:\n",
    "                                audio_filename = line[:space_idx]        # output audio file name = utterance name\n",
    "                                text = line[timestampEndBracket_idx+3:]         # the transcription of the utterance\n",
    "                                evaluation = evaluations_per_utterance.get(audio_filename,\"KEY_ERROR\")\n",
    "                                if(evaluation==\"KEY_ERROR\"):\n",
    "                                    self.errors[\"Unavailable Label for an utterance\"]+=1\n",
    "\n",
    "                                utterance_audios_per_video_folder = audio_filename[:line.rindex('_')]       # Only need Ses01F_impro01 from Ses01F_impro01_F000\n",
    "                                audio_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.AUDIO_FOLDER, utterance_audios_per_video_folder, audio_filename+\".wav\")         # name of the video file\n",
    "\n",
    "                                if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True and self.is_label_a_closed_label(evaluation)==self.is_closed_label_set_flag):\n",
    "                                # if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True):    \n",
    "                                    self.audio_files.append(audio_file_full_path)\n",
    "                                    self.sentences_list.append(text)\n",
    "                                    dataset.append((text,audio_file_full_path,evaluation))\n",
    "                                    if evaluation not in self.unique_labels:\n",
    "                                        self.unique_labels.append(evaluation)\n",
    "        return dataset\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"SUMMARY:\\n\")\n",
    "        for k,v in self.errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    def create_spectrograms(self,iemocap_spectrogram_dir):\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_spectrogram_dir)\n",
    "        log_file_path = os.path.join(log_dir,'processed_files_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir,'error_files_spectrogram.log')\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        throttle_delay = 1 \n",
    "        def create_spectrogram(filename, audio_file_path, output_file_path):\n",
    "            y, sr = librosa.load(audio_file_path)\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(S_dB, sr=sr, fmax=8000)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_file_path)\n",
    "            plt.close()\n",
    "\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filename = self.audio_files[filenum]\n",
    "            if filename.endswith(\".wav\") and filename not in processed_files:\n",
    "\n",
    "                audio_file_path = os.path.join(filename)\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0])\n",
    "                try:\n",
    "                    create_spectrogram(filename, audio_file_path, output_file_path)\n",
    "                    processed_files.add(filename)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filename}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filename}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    \n",
    "    def create_log_spectrograms(self,iemocap_log_spectrogram_dir):\n",
    "        def log_specgram(audio, sample_rate, window_size=20,\n",
    "                        step_size=10, eps=1e-10):\n",
    "            nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "            noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "            freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', \n",
    "                                                    nperseg=nperseg, noverlap=noverlap, detrend=False)\n",
    "            return freqs, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "        def process_audio_file(filepath, output_dir):\n",
    "            sample_rate, audio = wavfile.read(filepath)\n",
    "            if audio.ndim > 1:\n",
    "                audio = audio.mean(axis=1)\n",
    "            _, spectrogram = log_specgram(audio, sample_rate)\n",
    "            plt.figure(figsize=(10, 4))  \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, os.path.splitext(os.path.basename(filepath))[0]+\".png\"))\n",
    "            plt.close()  \n",
    "\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_log_spectrogram_dir)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        log_file_path = os.path.join(log_dir, 'processed_files_log_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir, 'error_files_log_spectrogram.log')\n",
    "\n",
    "        throttle_delay = 1 \n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filepath = self.audio_files[filenum]\n",
    "            if filepath.endswith(\".wav\") and filepath not in processed_files:\n",
    "                try:\n",
    "                    process_audio_file(filepath, output_dir)\n",
    "                    processed_files.add(filepath)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filepath}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filepath}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for log spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_t =  preprocessor(img).to(device)\n",
    "        return img_t\n",
    "\n",
    "    def extract_audio_features_from_spectrogram(self, img):\n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            output = self.feature_extractor(img)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    from functools import lru_cache\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_audio_features(self, img_path):\n",
    "        spectrogram_data = Image.open(img_path)\n",
    "        spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "        spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "        spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "        return spectrogram_data\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, audio, label = self.dataset[idx]\n",
    "\n",
    "        text = self.sentence_embeddings[idx]\n",
    "\n",
    "        img_path = os.path.join(os.path.dirname(os.getcwd()),self.iemocap_spectrogram_dir,os.path.splitext(os.path.basename(audio))[0]+\".png\")\n",
    "        spectrogram_data = self.cached_audio_features(img_path)\n",
    "\n",
    "        if(self.is_closed_label_set_flag==False):\n",
    "            label = OTHER_LABEL\n",
    "        else:\n",
    "            label = self.labels_to_int[label]\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return text, spectrogram_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "100%|██████████| 245/245 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:00<00:00, 245134.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 77/77 [00:10<00:00,  7.58it/s]\n",
      "100%|██████████| 9794/9794 [00:00<00:00, 1637201.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9794/9794 [00:00<00:00, 1964281.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    }
   ],
   "source": [
    "IEMOCAP_FULL_PATH = os.path.join(os.path.dirname(os.getcwd()),\"IEMOCAP_full_release\")\n",
    "labels_to_int = {'Neutral state': 0,\n",
    "                'Frustration': 1,\n",
    "                'Anger': 2,\n",
    "                'Sadness': 3,\n",
    "                'Happiness': 4,\n",
    "                'Excited': 5,\n",
    "                'Surprise': 6,\n",
    "                'Fear': 7,\n",
    "                'Other': 8,\n",
    "                'Disgust': 9}\n",
    "\n",
    "openIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=False,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)\n",
    "closedIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=True,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2916, 5: 1976, 0: 1726, 2: 1269, 3: 1251, 4: 656})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([i[2] for i in closedIemocapDataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9794"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(closedIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_split(dataset, test_size):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset_labels = [item[2] for item in dataset]\n",
    "    stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Get the indices for training and testing sets\n",
    "    for train_idx, test_idx in stratified_splitter.split(indices, dataset_labels):\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, temp_dataset = get_stratified_split(closedIemocapDataset,0.2)\n",
    "val_dataset_1, test_dataset_1 = get_stratified_split(temp_dataset,0.5)\n",
    "val_dataset_2, test_dataset_2 = get_stratified_split(openIemocapDataset,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset_1+val_dataset_2\n",
    "test_dataset = test_dataset_1+test_dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTextEmotionModel(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AudioTextEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioTextEmotionModel, self).__init__()\n",
    "        # sequential model with 2 layers, followed by dropout and relu layers and output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 768, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        combined = torch.cat([audio, text], axis=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "\n",
    "model = AudioTextEmotionModel(7)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.949139  [   64/ 7835]\n",
      "loss: 1.681185  [ 6464/ 7835]\n",
      "0.4289725590299936\n",
      "0.3372620126926564\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.426537  [   64/ 7835]\n",
      "loss: 1.432059  [ 6464/ 7835]\n",
      "0.4816847479259732\n",
      "0.3644605621033545\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.362325  [   64/ 7835]\n",
      "loss: 1.295874  [ 6464/ 7835]\n",
      "0.5324824505424378\n",
      "0.37262012692656393\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.237118  [   64/ 7835]\n",
      "loss: 1.169162  [ 6464/ 7835]\n",
      "0.5674537332482451\n",
      "0.3825929283771532\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.169981  [   64/ 7835]\n",
      "loss: 0.900950  [ 6464/ 7835]\n",
      "0.6131461391193364\n",
      "0.4016319129646419\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.940765  [   64/ 7835]\n",
      "loss: 0.841339  [ 6464/ 7835]\n",
      "0.6695596681557116\n",
      "0.4215775158658205\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.749401  [   64/ 7835]\n",
      "loss: 0.744649  [ 6464/ 7835]\n",
      "0.6630504148053605\n",
      "0.4016319129646419\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.710759  [   64/ 7835]\n",
      "loss: 0.731777  [ 6464/ 7835]\n",
      "0.7467772814294831\n",
      "0.4170444242973708\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.539950  [   64/ 7835]\n",
      "loss: 0.622509  [ 6464/ 7835]\n",
      "0.7891512444160816\n",
      "0.3970988213961922\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.684003  [   64/ 7835]\n",
      "loss: 0.420922  [ 6464/ 7835]\n",
      "0.8071474154435226\n",
      "0.41069809610154123\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.536111  [   64/ 7835]\n",
      "loss: 0.335797  [ 6464/ 7835]\n",
      "0.8070197830248883\n",
      "0.4252039891205802\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.335951  [   64/ 7835]\n",
      "loss: 0.369826  [ 6464/ 7835]\n",
      "0.8601148691767709\n",
      "0.42883046237534\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.287595  [   64/ 7835]\n",
      "loss: 0.301215  [ 6464/ 7835]\n",
      "0.803446075303127\n",
      "0.3916591115140526\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.282051  [   64/ 7835]\n",
      "loss: 0.289009  [ 6464/ 7835]\n",
      "0.8610082961072112\n",
      "0.3952855847688123\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.474847  [   64/ 7835]\n",
      "loss: 0.207790  [ 6464/ 7835]\n",
      "0.84594767070836\n",
      "0.4097914777878513\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.355532  [   64/ 7835]\n",
      "loss: 0.258954  [ 6464/ 7835]\n",
      "0.8973835354179962\n",
      "0.41069809610154123\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.164287  [   64/ 7835]\n",
      "loss: 0.190335  [ 6464/ 7835]\n",
      "0.8986598596043395\n",
      "0.42067089755213055\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.356359  [   64/ 7835]\n",
      "loss: 0.163507  [ 6464/ 7835]\n",
      "0.9209955328653477\n",
      "0.4460562103354488\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.220751  [   64/ 7835]\n",
      "loss: 0.322049  [ 6464/ 7835]\n",
      "0.9040204211869814\n",
      "0.3943789664551224\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.360393  [   64/ 7835]\n",
      "loss: 0.243682  [ 6464/ 7835]\n",
      "0.9183152520740268\n",
      "0.41795104261106075\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.267091  [   64/ 7835]\n",
      "loss: 0.102220  [ 6464/ 7835]\n",
      "0.9206126356094448\n",
      "0.41795104261106075\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.307230  [   64/ 7835]\n",
      "loss: 0.143122  [ 6464/ 7835]\n",
      "0.8922782386726228\n",
      "0.4242973708068903\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.215557  [   64/ 7835]\n",
      "loss: 0.155439  [ 6464/ 7835]\n",
      "0.9388640714741544\n",
      "0.42883046237534\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.229571  [   64/ 7835]\n",
      "loss: 0.158574  [ 6464/ 7835]\n",
      "0.9386088066368857\n",
      "0.4188576609247507\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.160334  [   64/ 7835]\n",
      "loss: 0.135040  [ 6464/ 7835]\n",
      "0.9443522654754307\n",
      "0.44514959202175886\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.119106  [   64/ 7835]\n",
      "loss: 0.088528  [ 6464/ 7835]\n",
      "0.9479259731971921\n",
      "0.4442429737080689\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.149580  [   64/ 7835]\n",
      "loss: 0.234543  [ 6464/ 7835]\n",
      "0.9411614550095724\n",
      "0.42883046237534\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.232715  [   64/ 7835]\n",
      "loss: 0.291932  [ 6464/ 7835]\n",
      "0.9534141671984684\n",
      "0.42067089755213055\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.152716  [   64/ 7835]\n",
      "loss: 0.048349  [ 6464/ 7835]\n",
      "0.9619655392469687\n",
      "0.443336355394379\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.046754  [   64/ 7835]\n",
      "loss: 0.220762  [ 6464/ 7835]\n",
      "0.9484365028717294\n",
      "0.41160471441523117\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.265543  [   64/ 7835]\n",
      "loss: 0.166566  [ 6464/ 7835]\n",
      "0.959029993618379\n",
      "0.4342701722574796\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.113001  [   64/ 7835]\n",
      "loss: 0.138476  [ 6464/ 7835]\n",
      "0.9544352265475431\n",
      "0.41160471441523117\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.062890  [   64/ 7835]\n",
      "loss: 0.085773  [ 6464/ 7835]\n",
      "0.9631142310146777\n",
      "0.44061650045330913\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.129431  [   64/ 7835]\n",
      "loss: 0.106650  [ 6464/ 7835]\n",
      "0.9657945118059987\n",
      "0.44514959202175886\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.082684  [   64/ 7835]\n",
      "loss: 0.068255  [ 6464/ 7835]\n",
      "0.9665603063178048\n",
      "0.43245693563009974\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.015296  [   64/ 7835]\n",
      "loss: 0.284964  [ 6464/ 7835]\n",
      "0.9687300574345884\n",
      "0.4514959202175884\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.159480  [   64/ 7835]\n",
      "loss: 0.145757  [ 6464/ 7835]\n",
      "0.9706445437141034\n",
      "0.4524025385312783\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.177219  [   64/ 7835]\n",
      "loss: 0.094569  [ 6464/ 7835]\n",
      "0.9733248245054243\n",
      "0.42883046237534\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.232331  [   64/ 7835]\n",
      "loss: 0.113444  [ 6464/ 7835]\n",
      "0.9738353541799617\n",
      "0.44152311876699907\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.055107  [   64/ 7835]\n",
      "loss: 0.008594  [ 6464/ 7835]\n",
      "0.9675813656668794\n",
      "0.4469628286491387\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.174274  [   64/ 7835]\n",
      "loss: 0.054050  [ 6464/ 7835]\n",
      "0.9665603063178048\n",
      "0.43517679057116954\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.116336  [   64/ 7835]\n",
      "loss: 0.209119  [ 6464/ 7835]\n",
      "0.972814294830887\n",
      "0.4342701722574796\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.046295  [   64/ 7835]\n",
      "loss: 0.052062  [ 6464/ 7835]\n",
      "0.97319719208679\n",
      "0.4614687216681777\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.308849  [   64/ 7835]\n",
      "loss: 0.016612  [ 6464/ 7835]\n",
      "0.973962986598596\n",
      "0.44786944696282865\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.176439  [   64/ 7835]\n",
      "loss: 0.264438  [ 6464/ 7835]\n",
      "0.9775366943203574\n",
      "0.43608340888485947\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.173404  [   64/ 7835]\n",
      "loss: 0.104005  [ 6464/ 7835]\n",
      "0.9761327377153797\n",
      "0.4315503173164098\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.122517  [   64/ 7835]\n",
      "loss: 0.042837  [ 6464/ 7835]\n",
      "0.976260370134014\n",
      "0.42883046237534\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.007011  [   64/ 7835]\n",
      "loss: 0.067880  [ 6464/ 7835]\n",
      "0.9748564135290364\n",
      "0.4397098821396192\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.055920  [   64/ 7835]\n",
      "loss: 0.056659  [ 6464/ 7835]\n",
      "0.9562220804084237\n",
      "0.4125113327289211\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.079858  [   64/ 7835]\n",
      "loss: 0.004368  [ 6464/ 7835]\n",
      "0.973962986598596\n",
      "0.4197642792384406\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.038818  [   64/ 7835]\n",
      "loss: 0.005309  [ 6464/ 7835]\n",
      "0.9749840459476707\n",
      "0.4242973708068903\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.020216  [   64/ 7835]\n",
      "loss: 0.003265  [ 6464/ 7835]\n",
      "0.9802169751116784\n",
      "0.44242973708068906\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.095895  [   64/ 7835]\n",
      "loss: 0.100858  [ 6464/ 7835]\n",
      "0.9857051691129547\n",
      "0.42611060743427015\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.052916  [   64/ 7835]\n",
      "loss: 0.193100  [ 6464/ 7835]\n",
      "0.9845564773452457\n",
      "0.47234814143245696\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.018469  [   64/ 7835]\n",
      "loss: 0.031753  [ 6464/ 7835]\n",
      "0.9802169751116784\n",
      "0.4397098821396192\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.021484  [   64/ 7835]\n",
      "loss: 0.152534  [ 6464/ 7835]\n",
      "0.9814932992980216\n",
      "0.4442429737080689\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.014721  [   64/ 7835]\n",
      "loss: 0.076203  [ 6464/ 7835]\n",
      "0.985067007019783\n",
      "0.43880326382592927\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.013969  [   64/ 7835]\n",
      "loss: 0.015768  [ 6464/ 7835]\n",
      "0.9886407147415444\n",
      "0.45058930190389845\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.013876  [   64/ 7835]\n",
      "loss: 0.004190  [ 6464/ 7835]\n",
      "0.982769623484365\n",
      "0.4397098821396192\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.006748  [   64/ 7835]\n",
      "loss: 0.156656  [ 6464/ 7835]\n",
      "0.9890236119974474\n",
      "0.4397098821396192\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.035953  [   64/ 7835]\n",
      "loss: 0.197751  [ 6464/ 7835]\n",
      "0.9795788130185067\n",
      "0.44152311876699907\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.010911  [   64/ 7835]\n",
      "loss: 0.023118  [ 6464/ 7835]\n",
      "0.982386726228462\n",
      "0.44061650045330913\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.011619  [   64/ 7835]\n",
      "loss: 0.108655  [ 6464/ 7835]\n",
      "0.981620931716656\n",
      "0.44061650045330913\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.118790  [   64/ 7835]\n",
      "loss: 0.006606  [ 6464/ 7835]\n",
      "0.9868538608806637\n",
      "0.4487760652765186\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.042332  [   64/ 7835]\n",
      "loss: 0.087784  [ 6464/ 7835]\n",
      "0.9849393746011487\n",
      "0.43517679057116954\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.048681  [   64/ 7835]\n",
      "loss: 0.006450  [ 6464/ 7835]\n",
      "0.9897894065092534\n",
      "0.4469628286491387\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.052721  [   64/ 7835]\n",
      "loss: 0.041574  [ 6464/ 7835]\n",
      "0.982003828972559\n",
      "0.45330915684496825\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.058123  [   64/ 7835]\n",
      "loss: 0.011322  [ 6464/ 7835]\n",
      "0.9880025526483727\n",
      "0.443336355394379\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.044777  [   64/ 7835]\n",
      "loss: 0.003385  [ 6464/ 7835]\n",
      "0.9894065092533504\n",
      "0.45330915684496825\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.069994  [   64/ 7835]\n",
      "loss: 0.065937  [ 6464/ 7835]\n",
      "0.987364390555201\n",
      "0.4551223934723481\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.085861  [   64/ 7835]\n",
      "loss: 0.137808  [ 6464/ 7835]\n",
      "0.9835354179961711\n",
      "0.44242973708068906\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.038405  [   64/ 7835]\n",
      "loss: 0.011807  [ 6464/ 7835]\n",
      "0.9802169751116784\n",
      "0.4215775158658205\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.014132  [   64/ 7835]\n",
      "loss: 0.017235  [ 6464/ 7835]\n",
      "0.9840459476707084\n",
      "0.44152311876699907\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.001599  [   64/ 7835]\n",
      "loss: 0.158030  [ 6464/ 7835]\n",
      "0.98851308232291\n",
      "0.45693563009972804\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.078078  [   64/ 7835]\n",
      "loss: 0.097823  [ 6464/ 7835]\n",
      "0.9945118059987237\n",
      "0.4551223934723481\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.002416  [   64/ 7835]\n",
      "loss: 0.185398  [ 6464/ 7835]\n",
      "0.9798340778557754\n",
      "0.4496826835902085\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.009051  [   64/ 7835]\n",
      "loss: 0.042427  [ 6464/ 7835]\n",
      "0.987747287811104\n",
      "0.44786944696282865\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.116594  [   64/ 7835]\n",
      "loss: 0.002276  [ 6464/ 7835]\n",
      "0.9883854499042757\n",
      "0.44061650045330913\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.039265  [   64/ 7835]\n",
      "loss: 0.007574  [ 6464/ 7835]\n",
      "0.9749840459476707\n",
      "0.4487760652765186\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.080416  [   64/ 7835]\n",
      "loss: 0.001973  [ 6464/ 7835]\n",
      "0.9891512444160817\n",
      "0.45058930190389845\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.093187  [   64/ 7835]\n",
      "loss: 0.001189  [ 6464/ 7835]\n",
      "0.9894065092533504\n",
      "0.4551223934723481\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.021939  [   64/ 7835]\n",
      "loss: 0.089693  [ 6464/ 7835]\n",
      "0.9851946394384173\n",
      "0.4542157751586582\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.074487  [   64/ 7835]\n",
      "loss: 0.004048  [ 6464/ 7835]\n",
      "0.990810465858328\n",
      "0.46872166817769717\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.002215  [   64/ 7835]\n",
      "loss: 0.032918  [ 6464/ 7835]\n",
      "0.990044671346522\n",
      "0.4542157751586582\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.003926  [   64/ 7835]\n",
      "loss: 0.026545  [ 6464/ 7835]\n",
      "0.989278876834716\n",
      "0.4469628286491387\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.013088  [   64/ 7835]\n",
      "loss: 0.213873  [ 6464/ 7835]\n",
      "0.9868538608806637\n",
      "0.45330915684496825\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.028263  [   64/ 7835]\n",
      "loss: 0.030144  [ 6464/ 7835]\n",
      "0.980472239948947\n",
      "0.43245693563009974\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.002273  [   64/ 7835]\n",
      "loss: 0.036779  [ 6464/ 7835]\n",
      "0.9874920229738353\n",
      "0.43880326382592927\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.063468  [   64/ 7835]\n",
      "loss: 0.007220  [ 6464/ 7835]\n",
      "0.9895341416719847\n",
      "0.44242973708068906\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.088515  [   64/ 7835]\n",
      "loss: 0.002278  [ 6464/ 7835]\n",
      "0.9813656668793873\n",
      "0.43064369900271987\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.006030  [   64/ 7835]\n",
      "loss: 0.004585  [ 6464/ 7835]\n",
      "0.984301212507977\n",
      "0.443336355394379\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.000640  [   64/ 7835]\n",
      "loss: 0.002261  [ 6464/ 7835]\n",
      "0.9906828334396937\n",
      "0.43789664551223934\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.001358  [   64/ 7835]\n",
      "loss: 0.081137  [ 6464/ 7835]\n",
      "0.9872367581365666\n",
      "0.43336355394378967\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.022454  [   64/ 7835]\n",
      "loss: 0.020239  [ 6464/ 7835]\n",
      "0.9848117421825143\n",
      "0.43789664551223934\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.002317  [   64/ 7835]\n",
      "loss: 0.006317  [ 6464/ 7835]\n",
      "0.9836630504148054\n",
      "0.4224841341795104\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.124783  [   64/ 7835]\n",
      "loss: 0.016924  [ 6464/ 7835]\n",
      "0.990427568602425\n",
      "0.443336355394379\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.016790  [   64/ 7835]\n",
      "loss: 0.008370  [ 6464/ 7835]\n",
      "0.9880025526483727\n",
      "0.41523118766999095\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.007727  [   64/ 7835]\n",
      "loss: 0.000601  [ 6464/ 7835]\n",
      "0.9920867900446714\n",
      "0.42973708068902994\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.081081  [   64/ 7835]\n",
      "loss: 0.002348  [ 6464/ 7835]\n",
      "0.9899170389278877\n",
      "0.43064369900271987\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.009209  [   64/ 7835]\n",
      "loss: 0.017283  [ 6464/ 7835]\n",
      "0.9940012763241863\n",
      "0.4496826835902085\n"
     ]
    }
   ],
   "source": [
    "def accuracy(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        predicted = torch.argmax(pred,dim=1).cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "    return total_correct/size\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        loss = loss_fn(pred, label)\n",
    "        # acc = accuracy_score(torch.argmax(pred,dim=1).cpu(), label.cpu())\n",
    "        # f1 = f1_score(torch.argmax(pred,dim=1).cpu(),label.cpu(),average=\"micro\")\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(text)\n",
    "            # print(f\"loss: {loss:>7f}\\t\\tAccuracy: {acc:>7f}\\t\\tF1 Score: {f1:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(accuracy(train_dataloader,model))\n",
    "    print(accuracy(test_dataloader,model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_labels = []\n",
    "# for text, spectrogram_data, label in test_dataloader:\n",
    "#     all_labels = all_labels + list(label)\n",
    "#     print (len(all_labels))\n",
    "#\n",
    "# Counter(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_dropout_to_train(eval_model):\n",
    "    for module in eval_model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def predict(label, model, text, spectrogram_data, n_simulations=100, threshold=0.5, other_label=OTHER_LABEL):\n",
    "    predictions = [torch.argmax(model(spectrogram_data, text), dim=1).float() for _ in range(n_simulations)]\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    # Calculate mode values along axis 0\n",
    "    mode_values, _ = torch.mode(predictions, dim=0)\n",
    "\n",
    "    # Calculate the count of each mode value\n",
    "    mode_counts = torch.zeros_like(mode_values)\n",
    "    for i, mode_val in enumerate(mode_values):\n",
    "        mode_counts[i] = torch.sum(predictions[:, i] == mode_val)\n",
    "\n",
    "    mode_values[mode_counts <= threshold*n_simulations] = other_label\n",
    "    actual_other_label = 0\n",
    "    correct_pred_of_other_label = 0\n",
    "    for i in range(len(label)):\n",
    "        l = label[i]\n",
    "        if l==other_label:\n",
    "            actual_other_label+=1\n",
    "            if mode_values[i]==6:\n",
    "                correct_pred_of_other_label+=1\n",
    "    \n",
    "    return mode_values, correct_pred_of_other_label, actual_other_label\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.6):\n",
    "    # After setting the model to evaluation mode, call this function\n",
    "    model.eval()\n",
    "    set_dropout_to_train(model)\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    total_correct_pred_of_other_label, total_actual_other_label = 0,0\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        predicted, correct_pred_other_label_for_this_batch, actual_other_label_for_this_batch = predict(label, model, text, spectrogram_data, threshold=threshold)\n",
    "        predicted = predicted.cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "        total_correct_pred_of_other_label+=correct_pred_other_label_for_this_batch\n",
    "        total_actual_other_label+=actual_other_label_for_this_batch\n",
    "\n",
    "    return total_correct/size, total_correct_pred_of_other_label/total_actual_other_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_accuracy, correct_other_labels = [],[]\n",
    "for threshold_val in np.logspace(-2,0,100,endpoint=True):\n",
    "    model_accuracy_for_this_threshold, correct_other_label_for_this_threshold = evaluate(model,val_dataloader,device,threshold=threshold_val)\n",
    "    model_accuracy.append(model_accuracy_for_this_threshold)\n",
    "    correct_other_labels.append(correct_other_label_for_this_threshold)\n",
    "\n",
    "plt.plot(range(len(model_accuracy)),model_accuracy,label=\"Accuracy\")\n",
    "plt.plot(range(len(correct_other_labels)),correct_other_labels,label=\"Correct other label\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3563009972801451, 0.42276422764227645)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,test_dataloader,device,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn pytorch basic with some basic models and datasets\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/nnqs_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
