{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Open Set Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_LABEL = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class MELDDataset(Dataset):\n",
    "#     def __init__(self, meld_dir, split, transform=None):\n",
    "#         train_df = pd.read_csv(\"../MELD_Dataset/train_sent_emo.csv\")\n",
    "#         labels = train_df['Emotion'].unique().tolist()\n",
    "#         self.label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "#         self.meld_dir = meld_dir\n",
    "#         self.transform = transform\n",
    "#         self.split = split\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'mel_spectrograms', f'{self.split}_img')\n",
    "#         self.img_path = os.path.join(self.meld_dir, 'log_spectrogram', f'{self.split}_audio')\n",
    "\n",
    "#         # load and create sentence embeddings\n",
    "#         self.dialogues = self.load_dialogues()\n",
    "#         self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "#         sentences = self.dialogues['Utterance'].tolist()\n",
    "#         sentences = [text.replace(\"\\x92\", \"'\") for text in sentences]\n",
    "#         self.sentence_embeddings = self.sbert.encode(sentences, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "\n",
    "#         self.spectrograms = self.load_spectrograms()\n",
    "#         self.resnet_model = models.resnet50(pretrained=True)\n",
    "#         self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "#         self.feature_extractor.eval()\n",
    "\n",
    "#     def load_dialogues(self):\n",
    "#         dialogue_file = os.path.join(self.meld_dir, f'{self.split}_sent_emo.csv')\n",
    "#         dialogues = pd.read_csv(dialogue_file)\n",
    "#         return dialogues\n",
    "\n",
    "#     def load_spectrograms(self):\n",
    "#         images = os.listdir(self.img_path)\n",
    "#         return images\n",
    "\n",
    "#     def __len__(self):\n",
    "#         assert(len(self.sentence_embeddings) == len(self.spectrograms))\n",
    "#         return len(self.dialogues)\n",
    "\n",
    "#     def preprocess_img(self, img):\n",
    "#         preprocessor = transforms.Compose([\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "#         img_t =  preprocessor(img).to(device)\n",
    "#         return img_t\n",
    "\n",
    "#     def extract_audio_features_from_spectrogram(self, img):\n",
    "#         # Pass the input through the model\n",
    "#         with torch.no_grad():\n",
    "#             output = self.feature_extractor(img)\n",
    "#         return output\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.dialogues.iloc[idx]\n",
    "#         text = self.sentence_embeddings[idx]\n",
    "#         spectrogram_data = Image.open(os.path.join(self.img_path, f'dia{row[\"Dialogue_ID\"]}_utt{row[\"Utterance_ID\"]}.png'))\n",
    "#         spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "#         spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "#         spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "#         spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "#         label = row['Emotion']\n",
    "#         label = torch.tensor(self.label_to_int[label])\n",
    "#         return text, spectrogram_data, label\n",
    "\n",
    "# train_meld = MELDDataset(\"../MELD_Dataset\", \"train\")\n",
    "# # test_meld = MELDDataset(\"../MELD_Dataset\", \"test\")\n",
    "# # dev_meld = MELDDataset(\"../MELD_Dataset\", \"dev\")\n",
    "\n",
    "# # concat all 3 datasets into 1 dataset\n",
    "# meld_dataset = train_meld # + test_meld + dev_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(meld_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IemocapDataset(Dataset):\n",
    "    def __init__(self, iemocap_dataset_full_path, iemocap_spectrogram_dir, iemocap_log_spectrogram_dir, is_closed_label_set_flag, labels_to_int, split, transform=None):\n",
    "        self.IEMOCAP_MAIN_FOLDER = os.path.join(iemocap_dataset_full_path,\"IEMOCAP_full_release\")\n",
    "        self.TRANSCRIPTION_FOLDER = os.path.join(\"dialog\", \"transcriptions\")\n",
    "        self.AUDIO_FOLDER = os.path.join(\"sentences\", \"wav\")\n",
    "        self.CATEGORICAL_LABELS_PATH = os.path.join(\"dialog\", \"EmoEvaluation\", \"Categorical\")\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.is_closed_label_set_flag = is_closed_label_set_flag\n",
    "        self.iemocap_spectrogram_dir = iemocap_spectrogram_dir\n",
    "        self.iemocap_log_spectrogram_dir = iemocap_log_spectrogram_dir\n",
    "        \n",
    "        self.errors = defaultdict(int)\n",
    "        self.unique_labels = []\n",
    "        self.audio_files = []\n",
    "        self.sentences_list = []\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.labels_to_int = labels_to_int\n",
    "\n",
    "        self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "        self.sentence_embeddings = self.sbert.encode(self.sentences_list, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "        \n",
    "        self.create_spectrograms(self.iemocap_spectrogram_dir)\n",
    "        self.create_log_spectrograms(self.iemocap_log_spectrogram_dir)\n",
    "        self.print_summary()\n",
    "\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "    def get_evaluator_filenames_with_video_file_prefix(self, input_list, prefix_value):\n",
    "        regex_pattern = re.compile(f'^{re.escape(prefix_value)}.*\\.txt$')\n",
    "        matching_strings = [s for s in input_list if regex_pattern.match(s)]\n",
    "        return matching_strings\n",
    "    \n",
    "    def get_utterance_to_evaluationCounter_mapping_from_evaluation_files(self, evaluation_files):\n",
    "        utterance_to_all_evaluations = {}\n",
    "\n",
    "        for evaluation_file in evaluation_files:\n",
    "            utterance_to_evaluationList = {}\n",
    "            with open(evaluation_file,'r') as f:\n",
    "                contents = f.read()\n",
    "                utterance_evaluations = contents.split(\"\\n\")\n",
    "                for evaluation in utterance_evaluations:\n",
    "                    evaluation = evaluation.strip()\n",
    "                    if(len(evaluation)==0):\n",
    "                        continue\n",
    "                    matches = re.findall(r':[^;]+;', evaluation)\n",
    "                    matches = [match[1:-1] for match in matches]\n",
    "                    utterance_to_evaluationList[evaluation.split()[0]] = matches\n",
    "            \n",
    "            # Combine lists from dict1\n",
    "            for key, value_list in utterance_to_evaluationList.items():\n",
    "                utterance_to_all_evaluations[key] = utterance_to_all_evaluations.get(key, []) + value_list\n",
    "\n",
    "        utterance_to_evaluationsCounter = {k:Counter(v).most_common(1)[0][0] for k,v in utterance_to_all_evaluations.items()}\n",
    "        return utterance_to_evaluationsCounter\n",
    "    \n",
    "    def is_label_a_closed_label(self,evaluation):\n",
    "        return evaluation in [\"Frustration\",\"Excited\",\"Neutral state\",\"Anger\",\"Sadness\",\"Happiness\"]\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for session_num in range(1,6):\n",
    "            for transcription_filename in os.listdir(os.path.join(self.IEMOCAP_MAIN_FOLDER,f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER)):\n",
    "                if(transcription_filename[0]!=\".\"): \n",
    "\n",
    "                    filename_without_extension = transcription_filename.split(\".\")[0]\n",
    "                    \n",
    "                    categorical_labels_folder_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH)\n",
    "                    evaluation_filenames = self.get_evaluator_filenames_with_video_file_prefix(os.listdir(categorical_labels_folder_full_path), filename_without_extension)\n",
    "                    evaluation_files_full_paths_for_this_file = [os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH, f) for f in evaluation_filenames]\n",
    "                    evaluations_per_utterance = self.get_utterance_to_evaluationCounter_mapping_from_evaluation_files(evaluation_files_full_paths_for_this_file)\n",
    "                    \n",
    "                    transcription_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER, transcription_filename) \n",
    "                    with open(transcription_file_full_path,'r') as f:\n",
    "                        contents = f.read()\n",
    "                        lines = contents.split(\"\\n\")\n",
    "\n",
    "                        # Iterate through utterances where every utterance looks like:\n",
    "                        # Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\n",
    "                        for line in lines:\n",
    "\n",
    "                            # Remove extra spaces and check if the line is not an empty link (usually at EOF)\n",
    "                            line = line.strip()\n",
    "                            if(len(line)==0):\n",
    "                                break\n",
    "\n",
    "                            # Remove idx of first space, ], -\n",
    "                            try:\n",
    "                                space_idx = line.index(\" \")\n",
    "                                timestampEndBracket_idx = line.index(\"]\")\n",
    "                                timestampHyphen_idx = line.index(\"-\")\n",
    "                            except:\n",
    "                                self.errors[\"Problematic Transcription Line\"]+=1\n",
    "                                continue\n",
    "                            else:\n",
    "                                audio_filename = line[:space_idx]        # output audio file name = utterance name\n",
    "                                text = line[timestampEndBracket_idx+3:]         # the transcription of the utterance\n",
    "                                evaluation = evaluations_per_utterance.get(audio_filename,\"KEY_ERROR\")\n",
    "                                if(evaluation==\"KEY_ERROR\"):\n",
    "                                    self.errors[\"Unavailable Label for an utterance\"]+=1\n",
    "\n",
    "                                utterance_audios_per_video_folder = audio_filename[:line.rindex('_')]       # Only need Ses01F_impro01 from Ses01F_impro01_F000\n",
    "                                audio_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.AUDIO_FOLDER, utterance_audios_per_video_folder, audio_filename+\".wav\")         # name of the video file\n",
    "\n",
    "                                if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True and self.is_label_a_closed_label(evaluation)==self.is_closed_label_set_flag):\n",
    "                                # if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True):    \n",
    "                                    self.audio_files.append(audio_file_full_path)\n",
    "                                    self.sentences_list.append(text)\n",
    "                                    dataset.append((text,audio_file_full_path,evaluation))\n",
    "                                    if evaluation not in self.unique_labels:\n",
    "                                        self.unique_labels.append(evaluation)\n",
    "        return dataset\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"SUMMARY:\\n\")\n",
    "        for k,v in self.errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    def create_spectrograms(self,iemocap_spectrogram_dir):\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_spectrogram_dir)\n",
    "        log_file_path = os.path.join(log_dir,'processed_files_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir,'error_files_spectrogram.log')\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        throttle_delay = 1 \n",
    "        def create_spectrogram(filename, audio_file_path, output_file_path):\n",
    "            y, sr = librosa.load(audio_file_path)\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(S_dB, sr=sr, fmax=8000)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_file_path)\n",
    "            plt.close()\n",
    "\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filename = self.audio_files[filenum]\n",
    "            if filename.endswith(\".wav\") and filename not in processed_files:\n",
    "\n",
    "                audio_file_path = os.path.join(filename)\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0])\n",
    "                try:\n",
    "                    create_spectrogram(filename, audio_file_path, output_file_path)\n",
    "                    processed_files.add(filename)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filename}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filename}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    \n",
    "    def create_log_spectrograms(self,iemocap_log_spectrogram_dir):\n",
    "        def log_specgram(audio, sample_rate, window_size=20,\n",
    "                        step_size=10, eps=1e-10):\n",
    "            nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "            noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "            freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', \n",
    "                                                    nperseg=nperseg, noverlap=noverlap, detrend=False)\n",
    "            return freqs, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "        def process_audio_file(filepath, output_dir):\n",
    "            sample_rate, audio = wavfile.read(filepath)\n",
    "            if audio.ndim > 1:\n",
    "                audio = audio.mean(axis=1)\n",
    "            _, spectrogram = log_specgram(audio, sample_rate)\n",
    "            plt.figure(figsize=(10, 4))  \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, os.path.splitext(os.path.basename(filepath))[0]+\".png\"))\n",
    "            plt.close()  \n",
    "\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_log_spectrogram_dir)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        log_file_path = os.path.join(log_dir, 'processed_files_log_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir, 'error_files_log_spectrogram.log')\n",
    "\n",
    "        throttle_delay = 1 \n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filepath = self.audio_files[filenum]\n",
    "            if filepath.endswith(\".wav\") and filepath not in processed_files:\n",
    "                try:\n",
    "                    process_audio_file(filepath, output_dir)\n",
    "                    processed_files.add(filepath)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filepath}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filepath}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for log spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_t =  preprocessor(img).to(device)\n",
    "        return img_t\n",
    "\n",
    "    def extract_audio_features_from_spectrogram(self, img):\n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            output = self.feature_extractor(img)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    from functools import lru_cache\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_audio_features(self, img_path):\n",
    "        spectrogram_data = Image.open(img_path)\n",
    "        spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "        spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "        spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "        return spectrogram_data\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, audio, label = self.dataset[idx]\n",
    "\n",
    "        text = self.sentence_embeddings[idx]\n",
    "\n",
    "        img_path = os.path.join(os.path.dirname(os.getcwd()),self.iemocap_spectrogram_dir,os.path.splitext(os.path.basename(audio))[0]+\".png\")\n",
    "        spectrogram_data = self.cached_audio_features(img_path)\n",
    "\n",
    "        if(self.is_closed_label_set_flag==False):\n",
    "            label = OTHER_LABEL\n",
    "        else:\n",
    "            label = self.labels_to_int[label]\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return text, spectrogram_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\n",
      "100%|██████████| 245/245 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:00<00:00, 245368.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 77/77 [00:10<00:00,  7.62it/s]\n",
      "100%|██████████| 9794/9794 [00:00<00:00, 1483586.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9794/9794 [00:00<00:00, 1630637.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    }
   ],
   "source": [
    "IEMOCAP_FULL_PATH = os.path.join(os.path.dirname(os.getcwd()),\"IEMOCAP_full_release\")\n",
    "labels_to_int = {'Neutral state': 0,\n",
    "                'Frustration': 1,\n",
    "                'Anger': 2,\n",
    "                'Sadness': 3,\n",
    "                'Happiness': 4,\n",
    "                'Excited': 5,\n",
    "                'Surprise': 6,\n",
    "                'Fear': 7,\n",
    "                'Other': 8,\n",
    "                'Disgust': 9}\n",
    "\n",
    "openIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=False,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)\n",
    "closedIemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=True,\n",
    "                                labels_to_int=labels_to_int,\n",
    "                                split=None,\n",
    "                                transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2916, 5: 1976, 0: 1726, 2: 1269, 3: 1251, 4: 656})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([i[2] for i in closedIemocapDataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9794"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(closedIemocapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_split(dataset, test_size):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset_labels = [item[2] for item in dataset]\n",
    "    stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Get the indices for training and testing sets\n",
    "    for train_idx, test_idx in stratified_splitter.split(indices, dataset_labels):\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, temp_dataset = get_stratified_split(closedIemocapDataset,0.2)\n",
    "val_dataset_1, test_dataset_1 = get_stratified_split(temp_dataset,0.5)\n",
    "val_dataset_2, test_dataset_2 = get_stratified_split(openIemocapDataset,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset_1+val_dataset_2\n",
    "test_dataset = test_dataset_1+test_dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTextEmotionModel(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AudioTextEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioTextEmotionModel, self).__init__()\n",
    "        # sequential model with 2 layers, followed by dropout and relu layers and output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 768, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        combined = torch.cat([audio, text], axis=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "\n",
    "model = AudioTextEmotionModel(6)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.859513  [   64/ 7835]\n",
      "loss: 1.640072  [ 6464/ 7835]\n",
      "0.4266751754945756\n",
      "0.34542157751586583\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.487526  [   64/ 7835]\n",
      "loss: 1.406634  [ 6464/ 7835]\n",
      "0.49917038927887686\n",
      "0.3744333635539438\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.311759  [   64/ 7835]\n",
      "loss: 1.212321  [ 6464/ 7835]\n",
      "0.5429483088704531\n",
      "0.3871260199456029\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.213040  [   64/ 7835]\n",
      "loss: 1.091915  [ 6464/ 7835]\n",
      "0.5899170389278877\n",
      "0.40344514959202177\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.133252  [   64/ 7835]\n",
      "loss: 0.943924  [ 6464/ 7835]\n",
      "0.613018506700702\n",
      "0.39619220308250225\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.946217  [   64/ 7835]\n",
      "loss: 0.849176  [ 6464/ 7835]\n",
      "0.650159540523293\n",
      "0.414324569356301\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.839177  [   64/ 7835]\n",
      "loss: 0.886581  [ 6464/ 7835]\n",
      "0.7045309508615188\n",
      "0.4097914777878513\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.926578  [   64/ 7835]\n",
      "loss: 0.673287  [ 6464/ 7835]\n",
      "0.6861518825781748\n",
      "0.39981867633726204\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.780834  [   64/ 7835]\n",
      "loss: 0.557211  [ 6464/ 7835]\n",
      "0.7627313337587748\n",
      "0.40797824116047143\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.586649  [   64/ 7835]\n",
      "loss: 0.692017  [ 6464/ 7835]\n",
      "0.78468410976388\n",
      "0.4161378059836809\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.485834  [   64/ 7835]\n",
      "loss: 0.305339  [ 6464/ 7835]\n",
      "0.7109125717932355\n",
      "0.38531278331822305\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.345344  [   64/ 7835]\n",
      "loss: 0.276028  [ 6464/ 7835]\n",
      "0.8232291001914487\n",
      "0.414324569356301\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.448563  [   64/ 7835]\n",
      "loss: 0.362863  [ 6464/ 7835]\n",
      "0.81914486279515\n",
      "0.4161378059836809\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.347127  [   64/ 7835]\n",
      "loss: 0.288539  [ 6464/ 7835]\n",
      "0.8693044033184428\n",
      "0.4070716228467815\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.329043  [   64/ 7835]\n",
      "loss: 0.239901  [ 6464/ 7835]\n",
      "0.8786215698787492\n",
      "0.4215775158658205\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.131656  [   64/ 7835]\n",
      "loss: 0.128248  [ 6464/ 7835]\n",
      "0.8834716017868539\n",
      "0.4170444242973708\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.157889  [   64/ 7835]\n",
      "loss: 0.195195  [ 6464/ 7835]\n",
      "0.8451818761965539\n",
      "0.4097914777878513\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.366231  [   64/ 7835]\n",
      "loss: 0.194115  [ 6464/ 7835]\n",
      "0.9095086151882578\n",
      "0.42611060743427015\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.171499  [   64/ 7835]\n",
      "loss: 0.237056  [ 6464/ 7835]\n",
      "0.9198468410976388\n",
      "0.42611060743427015\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.141588  [   64/ 7835]\n",
      "loss: 0.100280  [ 6464/ 7835]\n",
      "0.9379706445437141\n",
      "0.4342701722574796\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.187996  [   64/ 7835]\n",
      "loss: 0.110046  [ 6464/ 7835]\n",
      "0.9352903637523932\n",
      "0.42973708068902994\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.091052  [   64/ 7835]\n",
      "loss: 0.136243  [ 6464/ 7835]\n",
      "0.9184428844926611\n",
      "0.40616500453309157\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.114958  [   64/ 7835]\n",
      "loss: 0.215413  [ 6464/ 7835]\n",
      "0.8957243139757498\n",
      "0.4252039891205802\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.283535  [   64/ 7835]\n",
      "loss: 0.221928  [ 6464/ 7835]\n",
      "0.9363114231014678\n",
      "0.4197642792384406\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.257688  [   64/ 7835]\n",
      "loss: 0.092216  [ 6464/ 7835]\n",
      "0.9315890236119975\n",
      "0.4188576609247507\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.166127  [   64/ 7835]\n",
      "loss: 0.194722  [ 6464/ 7835]\n",
      "0.9465220165922145\n",
      "0.4397098821396192\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.088452  [   64/ 7835]\n",
      "loss: 0.101363  [ 6464/ 7835]\n",
      "0.958647096362476\n",
      "0.4487760652765186\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.080925  [   64/ 7835]\n",
      "loss: 0.133811  [ 6464/ 7835]\n",
      "0.9370772176132738\n",
      "0.4487760652765186\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.156466  [   64/ 7835]\n",
      "loss: 0.129817  [ 6464/ 7835]\n",
      "0.9600510529674537\n",
      "0.4496826835902085\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.090649  [   64/ 7835]\n",
      "loss: 0.303783  [ 6464/ 7835]\n",
      "0.9688576898532227\n",
      "0.43880326382592927\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.078876  [   64/ 7835]\n",
      "loss: 0.015089  [ 6464/ 7835]\n",
      "0.970134014039566\n",
      "0.43608340888485947\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.083700  [   64/ 7835]\n",
      "loss: 0.179605  [ 6464/ 7835]\n",
      "0.9692405871091258\n",
      "0.4496826835902085\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.025523  [   64/ 7835]\n",
      "loss: 0.208751  [ 6464/ 7835]\n",
      "0.9576260370134014\n",
      "0.4342701722574796\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.026651  [   64/ 7835]\n",
      "loss: 0.049943  [ 6464/ 7835]\n",
      "0.9331206126356094\n",
      "0.414324569356301\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.141685  [   64/ 7835]\n",
      "loss: 0.136241  [ 6464/ 7835]\n",
      "0.9554562858966178\n",
      "0.4369900271985494\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.056325  [   64/ 7835]\n",
      "loss: 0.114658  [ 6464/ 7835]\n",
      "0.959412890874282\n",
      "0.43608340888485947\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.076299  [   64/ 7835]\n",
      "loss: 0.108875  [ 6464/ 7835]\n",
      "0.9730695596681557\n",
      "0.44152311876699907\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.030440  [   64/ 7835]\n",
      "loss: 0.045431  [ 6464/ 7835]\n",
      "0.9693682195277601\n",
      "0.43336355394378967\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.135868  [   64/ 7835]\n",
      "loss: 0.052505  [ 6464/ 7835]\n",
      "0.9767708998085514\n",
      "0.43608340888485947\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.075465  [   64/ 7835]\n",
      "loss: 0.049188  [ 6464/ 7835]\n",
      "0.9757498404594767\n",
      "0.443336355394379\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.064456  [   64/ 7835]\n",
      "loss: 0.039816  [ 6464/ 7835]\n",
      "0.9686024250159541\n",
      "0.43608340888485947\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.035722  [   64/ 7835]\n",
      "loss: 0.144853  [ 6464/ 7835]\n",
      "0.9742182514358647\n",
      "0.43880326382592927\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.020625  [   64/ 7835]\n",
      "loss: 0.183625  [ 6464/ 7835]\n",
      "0.9646458200382897\n",
      "0.41523118766999095\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.242066  [   64/ 7835]\n",
      "loss: 0.051588  [ 6464/ 7835]\n",
      "0.9754945756222081\n",
      "0.4342701722574796\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.059234  [   64/ 7835]\n",
      "loss: 0.077720  [ 6464/ 7835]\n",
      "0.9688576898532227\n",
      "0.45330915684496825\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.025884  [   64/ 7835]\n",
      "loss: 0.150094  [ 6464/ 7835]\n",
      "0.976643267389917\n",
      "0.43517679057116954\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.018463  [   64/ 7835]\n",
      "loss: 0.022553  [ 6464/ 7835]\n",
      "0.9780472239948947\n",
      "0.4542157751586582\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.020206  [   64/ 7835]\n",
      "loss: 0.060598  [ 6464/ 7835]\n",
      "0.974345883854499\n",
      "0.443336355394379\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.101529  [   64/ 7835]\n",
      "loss: 0.030928  [ 6464/ 7835]\n",
      "0.9634971282705808\n",
      "0.42067089755213055\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.030869  [   64/ 7835]\n",
      "loss: 0.038056  [ 6464/ 7835]\n",
      "0.9573707721761328\n",
      "0.42792384406165007\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.038536  [   64/ 7835]\n",
      "loss: 0.094692  [ 6464/ 7835]\n",
      "0.9721761327377154\n",
      "0.44152311876699907\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.009374  [   64/ 7835]\n",
      "loss: 0.009255  [ 6464/ 7835]\n",
      "0.9738353541799617\n",
      "0.4369900271985494\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.019474  [   64/ 7835]\n",
      "loss: 0.073614  [ 6464/ 7835]\n",
      "0.982769623484365\n",
      "0.42973708068902994\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.058225  [   64/ 7835]\n",
      "loss: 0.017766  [ 6464/ 7835]\n",
      "0.9859604339502234\n",
      "0.43880326382592927\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.016670  [   64/ 7835]\n",
      "loss: 0.025604  [ 6464/ 7835]\n",
      "0.9799617102744097\n",
      "0.4442429737080689\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.031610  [   64/ 7835]\n",
      "loss: 0.016596  [ 6464/ 7835]\n",
      "0.979323548181238\n",
      "0.44061650045330913\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.038109  [   64/ 7835]\n",
      "loss: 0.016913  [ 6464/ 7835]\n",
      "0.9738353541799617\n",
      "0.4460562103354488\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.047154  [   64/ 7835]\n",
      "loss: 0.131847  [ 6464/ 7835]\n",
      "0.9851946394384173\n",
      "0.44514959202175886\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.012340  [   64/ 7835]\n",
      "loss: 0.195408  [ 6464/ 7835]\n",
      "0.9832801531589024\n",
      "0.44514959202175886\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.055029  [   64/ 7835]\n",
      "loss: 0.101975  [ 6464/ 7835]\n",
      "0.9830248883216337\n",
      "0.4397098821396192\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.002091  [   64/ 7835]\n",
      "loss: 0.002280  [ 6464/ 7835]\n",
      "0.9871091257179323\n",
      "0.4542157751586582\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.039757  [   64/ 7835]\n",
      "loss: 0.006004  [ 6464/ 7835]\n",
      "0.9747287811104021\n",
      "0.42067089755213055\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.245275  [   64/ 7835]\n",
      "loss: 0.017659  [ 6464/ 7835]\n",
      "0.982003828972559\n",
      "0.44242973708068906\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.005360  [   64/ 7835]\n",
      "loss: 0.010143  [ 6464/ 7835]\n",
      "0.9692405871091258\n",
      "0.4043517679057117\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.024678  [   64/ 7835]\n",
      "loss: 0.028135  [ 6464/ 7835]\n",
      "0.9814932992980216\n",
      "0.43245693563009974\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.187087  [   64/ 7835]\n",
      "loss: 0.055105  [ 6464/ 7835]\n",
      "0.9799617102744097\n",
      "0.42611060743427015\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.021095  [   64/ 7835]\n",
      "loss: 0.100734  [ 6464/ 7835]\n",
      "0.9880025526483727\n",
      "0.44514959202175886\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.044725  [   64/ 7835]\n",
      "loss: 0.018592  [ 6464/ 7835]\n",
      "0.980089342693044\n",
      "0.43880326382592927\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.022674  [   64/ 7835]\n",
      "loss: 0.041149  [ 6464/ 7835]\n",
      "0.982386726228462\n",
      "0.4524025385312783\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.001632  [   64/ 7835]\n",
      "loss: 0.001331  [ 6464/ 7835]\n",
      "0.9817485641352903\n",
      "0.443336355394379\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.008120  [   64/ 7835]\n",
      "loss: 0.054647  [ 6464/ 7835]\n",
      "0.9818761965539247\n",
      "0.4342701722574796\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.014012  [   64/ 7835]\n",
      "loss: 0.098771  [ 6464/ 7835]\n",
      "0.985067007019783\n",
      "0.4487760652765186\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.038598  [   64/ 7835]\n",
      "loss: 0.032926  [ 6464/ 7835]\n",
      "0.9721761327377154\n",
      "0.43880326382592927\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.023575  [   64/ 7835]\n",
      "loss: 0.036012  [ 6464/ 7835]\n",
      "0.98468410976388\n",
      "0.4469628286491387\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.119239  [   64/ 7835]\n",
      "loss: 0.002307  [ 6464/ 7835]\n",
      "0.9803446075303127\n",
      "0.44061650045330913\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.030688  [   64/ 7835]\n",
      "loss: 0.003405  [ 6464/ 7835]\n",
      "0.9813656668793873\n",
      "0.45058930190389845\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.079008  [   64/ 7835]\n",
      "loss: 0.021735  [ 6464/ 7835]\n",
      "0.982003828972559\n",
      "0.43789664551223934\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.029050  [   64/ 7835]\n",
      "loss: 0.019378  [ 6464/ 7835]\n",
      "0.9869814932992981\n",
      "0.45058930190389845\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.005629  [   64/ 7835]\n",
      "loss: 0.085891  [ 6464/ 7835]\n",
      "0.9867262284620294\n",
      "0.4442429737080689\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.016887  [   64/ 7835]\n",
      "loss: 0.004724  [ 6464/ 7835]\n",
      "0.9853222718570517\n",
      "0.4614687216681777\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.002593  [   64/ 7835]\n",
      "loss: 0.016985  [ 6464/ 7835]\n",
      "0.9765156349712827\n",
      "0.4460562103354488\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.002914  [   64/ 7835]\n",
      "loss: 0.047118  [ 6464/ 7835]\n",
      "0.985067007019783\n",
      "0.457842248413418\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.095886  [   64/ 7835]\n",
      "loss: 0.001536  [ 6464/ 7835]\n",
      "0.9883854499042757\n",
      "0.443336355394379\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.055800  [   64/ 7835]\n",
      "loss: 0.000591  [ 6464/ 7835]\n",
      "0.9871091257179323\n",
      "0.4605621033544878\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000365  [   64/ 7835]\n",
      "loss: 0.003736  [ 6464/ 7835]\n",
      "0.9863433312061264\n",
      "0.44242973708068906\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.000928  [   64/ 7835]\n",
      "loss: 0.027176  [ 6464/ 7835]\n",
      "0.9867262284620294\n",
      "0.4524025385312783\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.004005  [   64/ 7835]\n",
      "loss: 0.067925  [ 6464/ 7835]\n",
      "0.9855775366943204\n",
      "0.45330915684496825\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.089676  [   64/ 7835]\n",
      "loss: 0.066197  [ 6464/ 7835]\n",
      "0.980472239948947\n",
      "0.44061650045330913\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.058251  [   64/ 7835]\n",
      "loss: 0.083382  [ 6464/ 7835]\n",
      "0.9835354179961711\n",
      "0.45330915684496825\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.000462  [   64/ 7835]\n",
      "loss: 0.323219  [ 6464/ 7835]\n",
      "0.9876196553924697\n",
      "0.43608340888485947\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.089550  [   64/ 7835]\n",
      "loss: 0.001506  [ 6464/ 7835]\n",
      "0.9883854499042757\n",
      "0.44786944696282865\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.049734  [   64/ 7835]\n",
      "loss: 0.003638  [ 6464/ 7835]\n",
      "0.9896617740906191\n",
      "0.45602901178603805\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.063471  [   64/ 7835]\n",
      "loss: 0.014945  [ 6464/ 7835]\n",
      "0.9867262284620294\n",
      "0.4524025385312783\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000603  [   64/ 7835]\n",
      "loss: 0.003499  [ 6464/ 7835]\n",
      "0.9929802169751116\n",
      "0.4487760652765186\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.098254  [   64/ 7835]\n",
      "loss: 0.004055  [ 6464/ 7835]\n",
      "0.989278876834716\n",
      "0.4496826835902085\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.002027  [   64/ 7835]\n",
      "loss: 0.002453  [ 6464/ 7835]\n",
      "0.9864709636247607\n",
      "0.44786944696282865\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.055161  [   64/ 7835]\n",
      "loss: 0.003452  [ 6464/ 7835]\n",
      "0.9899170389278877\n",
      "0.44152311876699907\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.000791  [   64/ 7835]\n",
      "loss: 0.054536  [ 6464/ 7835]\n",
      "0.9897894065092534\n",
      "0.4496826835902085\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.064355  [   64/ 7835]\n",
      "loss: 0.066969  [ 6464/ 7835]\n",
      "0.987747287811104\n",
      "0.44061650045330913\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.000648  [   64/ 7835]\n",
      "loss: 0.008922  [ 6464/ 7835]\n",
      "0.9883854499042757\n",
      "0.4514959202175884\n"
     ]
    }
   ],
   "source": [
    "def accuracy(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        predicted = torch.argmax(pred,dim=1).cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "    return total_correct/size\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(spectrogram_data, text)\n",
    "        loss = loss_fn(pred, label)\n",
    "        # acc = accuracy_score(torch.argmax(pred,dim=1).cpu(), label.cpu())\n",
    "        # f1 = f1_score(torch.argmax(pred,dim=1).cpu(),label.cpu(),average=\"micro\")\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(text)\n",
    "            # print(f\"loss: {loss:>7f}\\t\\tAccuracy: {acc:>7f}\\t\\tF1 Score: {f1:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(accuracy(train_dataloader,model))\n",
    "    print(accuracy(test_dataloader,model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 62.,  47.,   4.,   7.,   6.,  16.,  12.],\n",
      "        [ 45., 124.,  54.,  21.,   7.,  26.,  36.],\n",
      "        [  1.,  38.,  50.,   2.,   3.,   4.,   8.],\n",
      "        [ 15.,  30.,   1.,  77.,  11.,  11.,  16.],\n",
      "        [  4.,   1.,   1.,   3.,  12.,  10.,   1.],\n",
      "        [ 37.,  36.,   9.,  10.,  33., 121.,  30.],\n",
      "        [  8.,  16.,   8.,   5.,   7.,   9.,   6.]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.410535876475931"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_dropout_to_train(eval_model):\n",
    "    for module in eval_model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def predict(label, model, text, spectrogram_data, n_simulations=100, threshold=1, other_label=OTHER_LABEL):\n",
    "    predictions = [model(spectrogram_data, text).detach().cpu() for _ in range(n_simulations)]\n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions = F.softmax(predictions, dim=2)\n",
    "\n",
    "    mean_predictions = torch.mean(predictions,dim=0)\n",
    "    std_predictions = torch.mean(torch.std(predictions,dim=0),dim=1)\n",
    "    _,predicted_class = torch.max(mean_predictions,1)\n",
    "    high_uncertainty = std_predictions>threshold\n",
    "    predicted_class[high_uncertainty]=other_label\n",
    "    return predicted_class\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.6):\n",
    "    # After setting the model to evaluation mode, call this function\n",
    "    model.eval()\n",
    "    set_dropout_to_train(model)\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    total_confusion_matrix = torch.zeros((7,7))\n",
    "    # total_correct_pred_of_other_label, total_actual_other_label = 0,0\n",
    "    for batch, (text, spectrogram_data, label) in enumerate(dataloader):\n",
    "        text, spectrogram_data, label = text.to(device), spectrogram_data.to(device), label.to(device)\n",
    "\n",
    "        predicted = predict(label, model, text, spectrogram_data, threshold=threshold)\n",
    "        predicted = predicted.cpu()\n",
    "        actual = label.cpu()\n",
    "        correct = predicted == actual\n",
    "        total_correct += correct.sum().item()\n",
    "        cm = confusion_matrix(predicted,actual)\n",
    "        if(cm.shape[0]!=7 and cm.shape[1]!=7):\n",
    "            row_of_zeros = np.zeros((7-cm.shape[0],cm.shape[1]))\n",
    "            array_with_row = np.concatenate((cm, row_of_zeros), axis=0)\n",
    "\n",
    "            # Add a column of zeros at the end\n",
    "            column_of_zeros = np.zeros((7, 7-cm.shape[1]))\n",
    "            array_with_row_and_column = np.concatenate((array_with_row, column_of_zeros), axis=1)\n",
    "            cm = array_with_row_and_column\n",
    "\n",
    "        total_confusion_matrix+= cm\n",
    "\n",
    "    print(total_confusion_matrix)\n",
    "    return total_correct/size\n",
    "\n",
    "evaluate(model,val_dataloader,device,threshold=0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 68.,  51.,   9.,   9.,   7.,  15.,  14.],\n",
      "        [ 40., 140.,  31.,  15.,   7.,  25.,  39.],\n",
      "        [  7.,  35.,  62.,   3.,   1.,   6.,   8.],\n",
      "        [ 12.,  14.,   2.,  79.,   6.,   8.,  13.],\n",
      "        [  3.,   6.,   1.,   7.,   7.,  14.,   3.],\n",
      "        [ 33.,  30.,  11.,   8.,  34., 122.,  38.],\n",
      "        [ 10.,  16.,  11.,   4.,   3.,   8.,   8.]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44061650045330913"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,test_dataloader,device,threshold=0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn pytorch basic with some basic models and datasets\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/basics/nnqs_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
