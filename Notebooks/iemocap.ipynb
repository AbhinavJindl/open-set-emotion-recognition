{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.2.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp38-cp38-win_amd64.whl (2704.3 MB)\n",
      "     ---------------------------------------- 2.7/2.7 GB 524.4 kB/s eta 0:00:00\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\aryan\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\aryan\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\aryan\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-m952_l8b\\\\a83478dd290d62d39538920c63624021a7eac9b6fabc8ef5c6de234f'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\aryan\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\aryan\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\aryan\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "# %pip install moviepy\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import librosa\n",
    "import librosa.display\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models, transforms\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "# %pip install -U sentence-transformers==2.5.1\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IemocapDataset(Dataset):\n",
    "    def __init__(self, iemocap_dataset_full_path, iemocap_spectrogram_dir, iemocap_log_spectrogram_dir, is_closed_label_set_flag, split, transform=None):\n",
    "        self.IEMOCAP_MAIN_FOLDER = os.path.join(iemocap_dataset_full_path,\"IEMOCAP_full_release\")\n",
    "        self.TRANSCRIPTION_FOLDER = os.path.join(\"dialog\", \"transcriptions\")\n",
    "        self.AUDIO_FOLDER = os.path.join(\"sentences\", \"wav\")\n",
    "        self.CATEGORICAL_LABELS_PATH = os.path.join(\"dialog\", \"EmoEvaluation\", \"Categorical\")\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.is_closed_label_set_flag = is_closed_label_set_flag\n",
    "        self.iemocap_spectrogram_dir = iemocap_spectrogram_dir\n",
    "        self.iemocap_log_spectrogram_dir = iemocap_log_spectrogram_dir\n",
    "        \n",
    "        self.errors = defaultdict(int)\n",
    "        self.unique_labels = []\n",
    "        self.audio_files = []\n",
    "        self.sentences_list = []\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.labels_to_int = {label:i for i,label in enumerate(self.unique_labels)}\n",
    "\n",
    "        self.sbert = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "        self.sentence_embeddings = self.sbert.encode(self.sentences_list, convert_to_tensor=True, show_progress_bar=True, batch_size=128, device=device)\n",
    "        \n",
    "        self.create_spectrograms(self.iemocap_spectrogram_dir)\n",
    "        self.create_log_spectrograms(self.iemocap_log_spectrogram_dir)\n",
    "        self.print_summary()\n",
    "\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(self.resnet_model.children())[:-1]).to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "    def get_evaluator_filenames_with_video_file_prefix(self, input_list, prefix_value):\n",
    "        regex_pattern = re.compile(f'^{re.escape(prefix_value)}.*\\.txt$')\n",
    "        matching_strings = [s for s in input_list if regex_pattern.match(s)]\n",
    "        return matching_strings\n",
    "    \n",
    "    def get_utterance_to_evaluationCounter_mapping_from_evaluation_files(self, evaluation_files):\n",
    "        utterance_to_all_evaluations = {}\n",
    "\n",
    "        for evaluation_file in evaluation_files:\n",
    "            utterance_to_evaluationList = {}\n",
    "            with open(evaluation_file,'r') as f:\n",
    "                contents = f.read()\n",
    "                utterance_evaluations = contents.split(\"\\n\")\n",
    "                for evaluation in utterance_evaluations:\n",
    "                    evaluation = evaluation.strip()\n",
    "                    if(len(evaluation)==0):\n",
    "                        continue\n",
    "                    matches = re.findall(r':[^;]+;', evaluation)\n",
    "                    matches = [match[1:-1] for match in matches]\n",
    "                    utterance_to_evaluationList[evaluation.split()[0]] = matches\n",
    "            \n",
    "            # Combine lists from dict1\n",
    "            for key, value_list in utterance_to_evaluationList.items():\n",
    "                utterance_to_all_evaluations[key] = utterance_to_all_evaluations.get(key, []) + value_list\n",
    "\n",
    "        utterance_to_evaluationsCounter = {k:Counter(v).most_common(1)[0][0] for k,v in utterance_to_all_evaluations.items()}\n",
    "        return utterance_to_evaluationsCounter\n",
    "    \n",
    "    def is_label_a_closed_label(self,evaluation):\n",
    "        return evaluation in [\"Frustration\",\"Excited\",\"Neutral state\",\"Anger\",\"Sadness\",\"Happiness\"]\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for session_num in range(1,6):\n",
    "            for transcription_filename in os.listdir(os.path.join(self.IEMOCAP_MAIN_FOLDER,f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER)):\n",
    "                if(transcription_filename[0]!=\".\"): \n",
    "\n",
    "                    filename_without_extension = transcription_filename.split(\".\")[0]\n",
    "                    \n",
    "                    categorical_labels_folder_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH)\n",
    "                    evaluation_filenames = self.get_evaluator_filenames_with_video_file_prefix(os.listdir(categorical_labels_folder_full_path), filename_without_extension)\n",
    "                    evaluation_files_full_paths_for_this_file = [os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.CATEGORICAL_LABELS_PATH, f) for f in evaluation_filenames]\n",
    "                    evaluations_per_utterance = self.get_utterance_to_evaluationCounter_mapping_from_evaluation_files(evaluation_files_full_paths_for_this_file)\n",
    "                    \n",
    "                    transcription_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.TRANSCRIPTION_FOLDER, transcription_filename) \n",
    "                    with open(transcription_file_full_path,'r') as f:\n",
    "                        contents = f.read()\n",
    "                        lines = contents.split(\"\\n\")\n",
    "\n",
    "                        # Iterate through utterances where every utterance looks like:\n",
    "                        # Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\n",
    "                        for line in lines:\n",
    "\n",
    "                            # Remove extra spaces and check if the line is not an empty link (usually at EOF)\n",
    "                            line = line.strip()\n",
    "                            if(len(line)==0):\n",
    "                                break\n",
    "\n",
    "                            # Remove idx of first space, ], -\n",
    "                            try:\n",
    "                                space_idx = line.index(\" \")\n",
    "                                timestampEndBracket_idx = line.index(\"]\")\n",
    "                                timestampHyphen_idx = line.index(\"-\")\n",
    "                            except:\n",
    "                                self.errors[\"Problematic Transcription Line\"]+=1\n",
    "                                continue\n",
    "                            else:\n",
    "                                audio_filename = line[:space_idx]        # output audio file name = utterance name\n",
    "                                text = line[timestampEndBracket_idx+3:]         # the transcription of the utterance\n",
    "                                evaluation = evaluations_per_utterance.get(audio_filename,\"KEY_ERROR\")\n",
    "                                if(evaluation==\"KEY_ERROR\"):\n",
    "                                    self.errors[\"Unavailable Label for an utterance\"]+=1\n",
    "\n",
    "                                utterance_audios_per_video_folder = audio_filename[:line.rindex('_')]       # Only need Ses01F_impro01 from Ses01F_impro01_F000\n",
    "                                audio_file_full_path = os.path.join(self.IEMOCAP_MAIN_FOLDER, f\"Session{session_num}\", self.AUDIO_FOLDER, utterance_audios_per_video_folder, audio_filename+\".wav\")         # name of the video file\n",
    "\n",
    "                                # if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True and self.is_label_a_closed_label(evaluation)==self.is_closed_label_set_flag):\n",
    "                                if(evaluation!=\"KEY_ERROR\" and os.path.isfile(audio_file_full_path)==True):    \n",
    "                                    self.audio_files.append(audio_file_full_path)\n",
    "                                    self.sentences_list.append(text)\n",
    "                                    dataset.append((text,audio_file_full_path,evaluation))\n",
    "                                    if evaluation not in self.unique_labels:\n",
    "                                        self.unique_labels.append(evaluation)\n",
    "        return dataset\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"SUMMARY:\\n\")\n",
    "        for k,v in self.errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    def create_spectrograms(self,iemocap_spectrogram_dir):\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_spectrogram_dir)\n",
    "        log_file_path = os.path.join(log_dir,'processed_files_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir,'error_files_spectrogram.log')\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        throttle_delay = 1 \n",
    "        def create_spectrogram(filename, audio_file_path, output_file_path):\n",
    "            y, sr = librosa.load(audio_file_path)\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(S_dB, sr=sr, fmax=8000)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_file_path)\n",
    "            plt.close()\n",
    "\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filename = self.audio_files[filenum]\n",
    "            if filename.endswith(\".wav\") and filename not in processed_files:\n",
    "\n",
    "                audio_file_path = os.path.join(filename)\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0])\n",
    "                try:\n",
    "                    create_spectrogram(filename, audio_file_path, output_file_path)\n",
    "                    processed_files.add(filename)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filename}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filename}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    \n",
    "    def create_log_spectrograms(self,iemocap_log_spectrogram_dir):\n",
    "        def log_specgram(audio, sample_rate, window_size=20,\n",
    "                        step_size=10, eps=1e-10):\n",
    "            nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "            noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "            freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', \n",
    "                                                    nperseg=nperseg, noverlap=noverlap, detrend=False)\n",
    "            return freqs, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "        def process_audio_file(filepath, output_dir):\n",
    "            sample_rate, audio = wavfile.read(filepath)\n",
    "            if audio.ndim > 1:\n",
    "                audio = audio.mean(axis=1)\n",
    "            _, spectrogram = log_specgram(audio, sample_rate)\n",
    "            plt.figure(figsize=(10, 4))  \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, os.path.splitext(os.path.basename(filepath))[0]+\".png\"))\n",
    "            plt.close()  \n",
    "\n",
    "        log_dir = os.path.join(os.path.dirname(os.getcwd()),'iemocap','log_dir')\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()),iemocap_log_spectrogram_dir)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        log_file_path = os.path.join(log_dir, 'processed_files_log_spectrogram.log')\n",
    "        error_log_path = os.path.join(log_dir, 'error_files_log_spectrogram.log')\n",
    "\n",
    "        throttle_delay = 1 \n",
    "\n",
    "        processed_files = set()\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                processed_files = set(file.read().splitlines())\n",
    "\n",
    "        processed_files_count = 0\n",
    "        for filenum in tqdm(range(len(self.audio_files))):\n",
    "            filepath = self.audio_files[filenum]\n",
    "            if filepath.endswith(\".wav\") and filepath not in processed_files:\n",
    "                try:\n",
    "                    process_audio_file(filepath, output_dir)\n",
    "                    processed_files.add(filepath)\n",
    "                    processed_files_count += 1\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f\"{filepath}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {e}\")\n",
    "                    with open(error_log_path, 'a') as error_log:\n",
    "                        error_log.write(f\"{filepath}: {e}\\n\")\n",
    "                finally:\n",
    "                    time.sleep(throttle_delay)\n",
    "\n",
    "        print(f\"Batch conversion completed for log spectrograms. Processed {processed_files_count} files.\")\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_t =  preprocessor(img).to(device)\n",
    "        return img_t\n",
    "\n",
    "    def extract_audio_features_from_spectrogram(self, img):\n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            output = self.feature_extractor(img)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, audio, label = self.dataset[idx]\n",
    "\n",
    "        text = self.sentence_embeddings[idx]\n",
    "\n",
    "        spectrogram_data = Image.open(os.path.join(os.path.dirname(os.getcwd()),self.iemocap_spectrogram_dir,os.path.splitext(os.path.basename(audio))[0]+\".png\"))\n",
    "        spectrogram_data = self.preprocess_img(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data[0:3, :, :]\n",
    "        spectrogram_data = spectrogram_data.unsqueeze(0)\n",
    "        spectrogram_data = self.extract_audio_features_from_spectrogram(spectrogram_data)\n",
    "        spectrogram_data = spectrogram_data.view(-1, 2048)[0]\n",
    "\n",
    "        label = self.labels_to_int[label]\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return text, spectrogram_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files from D:\\Projects\\open-set-emotion-recognition\\IEMOCAP_full_release\\IEMOCAP_full_release\\SessionX\\dialog\\transcriptions (Note the SessionX)\n",
    "\n",
    "# In each of these files, you will get => Video file name, Timestamp, Text of each utterance\n",
    "\n",
    "# Get the audio clip based on the video file name and timestamp. Save the audio file in a folder and return librosa audio, text\n",
    "\n",
    "# To get emotion,  there are multiple evaluator files in D:\\Projects\\open-set-emotion-recognition\\IEMOCAP_full_release\\IEMOCAP_full_release\\Session1\\dialog\\EmoEvaluation\\Categorical\n",
    "# for 1 video file. So read all the \"TXT\" files corresp to 1 particular video file and get the majority label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [00:15<00:00,  5.15it/s]\n",
      "100%|██████████| 10039/10039 [00:00<00:00, 2515479.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for spectrograms. Processed 0 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10039/10039 [00:00<00:00, 3356446.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch conversion completed for log spectrograms. Processed 0 files.\n",
      "SUMMARY:\n",
      "\n",
      "Problematic Transcription Line: 152\n",
      "Unavailable Label for an utterance: 48\n"
     ]
    }
   ],
   "source": [
    "IEMOCAP_FULL_PATH = os.path.join(os.path.dirname(os.getcwd()),\"IEMOCAP_full_release\")\n",
    "\n",
    "iemocapDataset = IemocapDataset(iemocap_dataset_full_path=IEMOCAP_FULL_PATH,\n",
    "                                iemocap_spectrogram_dir=os.path.join(\"iemocap\",\"spectrogram\"),\n",
    "                                iemocap_log_spectrogram_dir=os.path.join(\"iemocap\",\"log_spectrogram\"),\n",
    "                                is_closed_label_set_flag=True,\n",
    "                                split=None,\n",
    "                                transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10039\n"
     ]
    }
   ],
   "source": [
    "print(len(iemocapDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768]) torch.Size([2048]) 0\n"
     ]
    }
   ],
   "source": [
    "print(iemocapDataset[0][0].shape, iemocapDataset[0][1].shape, iemocapDataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral state', 'Frustration', 'Anger', 'Sadness', 'Happiness', 'Surprise', 'Excited', 'Fear', 'Other', 'Disgust']\n"
     ]
    }
   ],
   "source": [
    "print(iemocapDataset.unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed_iemo = CLosedIEMO()\n",
    "# ope iemo = OPenIEMO()\n",
    "\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(cloed itemo, [0.8%, 0.2%])\n",
    "\n",
    "# val_dataset, test_dataset = torch.utils.data.random_split(test dataset, [0.5, 0.5])\n",
    "\n",
    "# val_dataset2, test_dataset2 = torch.utils.data.random_split(open iemo, [0.5, 0.5])\n",
    "\n",
    "# val = val_dataset + val_dataset2\n",
    "# test = test_dataset + test_dataset2\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
