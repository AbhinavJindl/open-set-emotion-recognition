{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84c9c3e-e03f-40f1-8009-67120f4d7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e656d9f-15f2-43fa-841d-b2a71736aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9989\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d745905-0dab-4d96-9e17-7433440dd230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540bad4c-f003-45e8-b050-33c25d7dfc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03e1be2-10d8-4aa3-98ab-f0db1cdc6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_dir = \"/path/to/meld_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0571cb88-fca2-4c55-ad20-afbf3875a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_dir = \"meld_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f91d0b0-a910-4bfe-91c3-531147694f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MELDDataset(meld_dir, split='train')\n",
    "dev_dataset = MELDDataset(meld_dir, split='dev')\n",
    "test_dataset = MELDDataset(meld_dir, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b76537-f081-416f-94a8-ec50c3d23a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import load\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, meld_dir, split, transform=None):\n",
    "        self.meld_dir = meld_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data\n",
    "        self.dialogues = self.load_dialogues()\n",
    "        \n",
    "    def load_dialogues(self):\n",
    "        dialogue_file = os.path.join(self.meld_dir, f'meld_{self.split}.csv')\n",
    "        dialogues = pd.read_csv(dialogue_file)\n",
    "        return dialogues\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dialogues.iloc[idx]\n",
    "        \n",
    "        # Extract dialogue information\n",
    "        text = row['Utterance']\n",
    "        audio_dir = os.path.join(self.meld_dir, 'wav', f'audio_files_for_{self.split}_set')\n",
    "        audio_file = os.path.join(audio_dir, f'{row[\"Dialogue_ID\"]}_{row[\"Utterance_ID\"]}.wav')\n",
    "        label = row['Emotion']\n",
    "        \n",
    "        try:\n",
    "            # Load audio data\n",
    "            audio_data, sample_rate = load(audio_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file: {str(e)}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Apply transformations if needed\n",
    "        if self.transform:\n",
    "            # Apply transformation to audio data\n",
    "            audio_data = self.transform(audio_data)\n",
    "\n",
    "        return text, audio_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027bcb09-c888-4ac2-9aec-657d4d76d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MELDDataset object at 0x7f816aea6d90>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c70f6519-f6f7-481e-a9e8-032b0f898124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MELDDataset object at 0x7f8158c8ccd0>\n"
     ]
    }
   ],
   "source": [
    "print(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fedf3b9-25b1-4743-b4f7-be72749fb0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MELDDataset object at 0x7f816ae84f70>\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "042b0732-c07c-4b35-89fe-31dea3c42810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9989\n",
      "9989\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179a9586-dab8-4300-927c-1a3f6070a529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109\n",
      "1109\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6720e28c-40f9-4e02-9a79-cd86950ba42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n",
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc8adfe-8801-4386-8197-2cb2c033e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neutral dialogues involving Ross: 719\n",
      "\n",
      "Sample of neutral dialogues involving Ross:\n",
      "    Sr No.                                          Utterance Speaker  \\\n",
      "46      50  Ah y'know, this building is on my paper route ...    Ross   \n",
      "48      52                                                Hi.    Ross   \n",
      "50      54                                   HowÂ’d did it go?    Ross   \n",
      "56      60                                              Yeah.    Ross   \n",
      "97     101                                            I know.    Ross   \n",
      "\n",
      "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
      "46  neutral   neutral            4             3       3       11   \n",
      "48  neutral   neutral            4             5       3       11   \n",
      "50  neutral   neutral            4             7       3       11   \n",
      "56  neutral   neutral            4            13       3       11   \n",
      "97  neutral   neutral            9             4       2        5   \n",
      "\n",
      "       StartTime       EndTime  \n",
      "46  00:20:26,433  00:20:29,185  \n",
      "48   0:20:30,771   0:20:30,873  \n",
      "50  00:20:31,188  00:20:32,230  \n",
      "56   0:20:54,962   0:20:55,380  \n",
      "97  00:02:13,091  00:02:14,758  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meld_dir = \"meld_dataset\"\n",
    "split = \"train\"\n",
    "dialogue_file = os.path.join(meld_dir, f'meld_{split}.csv')\n",
    "meld_df = pd.read_csv(dialogue_file)\n",
    "\n",
    "ross_dialogues = meld_df[meld_df['Speaker'] == 'Ross']\n",
    "\n",
    "ross_neutral_dialogues = ross_dialogues[ross_dialogues['Emotion'] == 'neutral']\n",
    "\n",
    "print(\"Number of neutral dialogues involving Ross:\", len(ross_neutral_dialogues))\n",
    "\n",
    "print(\"\\nSample of neutral dialogues involving Ross:\")\n",
    "print(ross_neutral_dialogues.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47f85920-1161-4aaf-9d9d-0c8335e45185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, meld_dir, split, transform=None):\n",
    "        self.meld_dir = meld_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data\n",
    "        self.dialogues = self.load_dialogues()\n",
    "        \n",
    "    def load_dialogues(self):\n",
    "        dialogue_file = os.path.join(self.meld_dir, f'meld_{self.split}.csv')\n",
    "        dialogues = pd.read_csv(dialogue_file)\n",
    "        return dialogues\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dialogues.iloc[idx]\n",
    "        \n",
    "        # Extract dialogue information\n",
    "        text = row['Utterance']\n",
    "        video_dir = os.path.join(self.meld_dir, 'wav', f'audio_files_for_{self.split}_set')\n",
    "        video_file = os.path.join(video_dir, f'dia{row[\"Dialogue_ID\"]}_utt{row[\"Utterance_ID\"]}.mp4')\n",
    "        label = row['Emotion']\n",
    "        \n",
    "        try:\n",
    "            # Load video data\n",
    "            video_data, audio_data, info = read_video(video_file, pts_unit='sec')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video file: {str(e)}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Extract audio track from video\n",
    "        audio_data = audio_data[:, 0]  # Extract first audio track\n",
    "        \n",
    "        # Apply transformations if needed\n",
    "        if self.transform:\n",
    "            # Apply transformation to audio data\n",
    "            audio_data = self.transform(audio_data)\n",
    "\n",
    "        return text, audio_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52924b77-9499-4dcb-96be-99bd28021222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import load\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, meld_dir, split, transform=None):\n",
    "        self.meld_dir = meld_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        self.dialogues = self.load_dialogues()\n",
    "        \n",
    "    def load_dialogues(self):\n",
    "        dialogue_file = os.path.join(self.meld_dir, f'meld_{self.split}.csv')\n",
    "        dialogues = pd.read_csv(dialogue_file)\n",
    "        return dialogues\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dialogues.iloc[idx]\n",
    "        \n",
    "        \n",
    "        text = row['Utterance']\n",
    "        audio_dir = os.path.join(self.meld_dir, 'wav', f'audio_files_for_{self.split}_set')\n",
    "        audio_file = os.path.join(audio_dir, f'{row[\"Dialogue_ID\"]}_{row[\"Utterance_ID\"]}.wav')\n",
    "        label = row['Emotion']\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            audio_data, sample_rate = load(audio_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file: {str(e)}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            \n",
    "            audio_data = self.transform(audio_data)\n",
    "\n",
    "        return text, audio_data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811394e-f24a-4801-a08b-d11b5da8722b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
